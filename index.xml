<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Medipixel</title>
    <link>/</link>
    <description>Recent content on Medipixel</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 01 May 2019 22:22:17 +0900</lastBuildDate>
    
	    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Anchor_notebook</title>
      <link>/post/anchor_notebook/</link>
      <pubDate>Wed, 01 May 2019 22:22:17 +0900</pubDate>
      
      <guid>/post/anchor_notebook/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch
import numpy as np
import cv2
import matplotlib.pyplot as plt
import matplotlib.ticker as plticker
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def gen_base_anchors(base_size, ratios, scales):
    w = base_size
    h = base_size
    x_ctr = 0.5 * (w - 1)
    y_ctr = 0.5 * (h - 1)


    h_ratios = torch.sqrt(ratios)
    w_ratios = 1 / h_ratios
    ws = (w * w_ratios[:, None] * scales[None, :]).view(-1)
    hs = (h * h_ratios[:, None] * scales[None, :]).view(-1)

    base_anchors = torch.stack(
        [
            x_ctr - 0.5 * (ws - 1), y_ctr - 0.5 * (hs - 1),
            x_ctr + 0.5 * (ws - 1), y_ctr + 0.5 * (hs - 1)
        ],
        dim=-1).round()

    return base_anchors
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def meshgrid(x, y):
    xx = x.repeat(len(y))
    yy = y.view(-1, 1).repeat(1, len(x)).view(-1)
    return xx, yy
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def grid_anchors(base_anchors, featmap_size, stride=16, device=&#39;cuda&#39;):
    base_anchors = base_anchors.to(device)

    feat_h, feat_w = featmap_size
    shift_x = torch.arange(0, feat_w, device=device) * stride
    shift_y = torch.arange(0, feat_h, device=device) * stride
    shift_xx, shift_yy = meshgrid(shift_x, shift_y)
    shifts = torch.stack([shift_xx, shift_yy, shift_xx, shift_yy], dim=-1)
    shifts = shifts.type_as(base_anchors)

    all_anchors = base_anchors[None, :, :] + shifts[:, None, :]
    all_anchors = all_anchors.view(-1, 4)
    return all_anchors, shifts
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def prepare_base_figure(shape, grid_size, figsize=(20, 20)):
    fig, ax = plt.subplots(figsize=figsize)

    loc = plticker.MultipleLocator(base=grid_size)
    ax.xaxis.set_major_locator(loc)
    ax.yaxis.set_major_locator(loc)

    ax.grid(which=&#39;major&#39;, axis=&#39;both&#39;, linestyle=&#39;--&#39;, color=&#39;w&#39;)
    return ax


def draw_anchor_gt_overlaps(overlaps, grid_size=1, figsize=(20, 20)):
    &amp;quot;&amp;quot;&amp;quot;Draw anchor overlaps w.r.t. gt bboxes&amp;quot;&amp;quot;&amp;quot;
    grid_x, grid_y = overlaps.shape[:2]
    ax = prepare_base_figure(overlaps.shape, grid_size, figsize)
    ax.imshow(overlaps, extent=[0, grid_x, 0, grid_y])
    plt.margins(0)
    plt.show()


def draw_pos_assigned_bboxes(image_shape, grid_size, gt_bboxes_list, pos_bboxes,
                             pos_pred_bboxes=None, figsize=(20, 20)):
    &amp;quot;&amp;quot;&amp;quot;Draw positive, negative bboxes.&amp;quot;&amp;quot;&amp;quot;
    background_image = np.zeros(image_shape)
    ax = prepare_base_figure(image_shape, grid_size, figsize)

    for gt_bbox in gt_bboxes_list:
        x1, y1, x2, y2 = gt_bbox
        cv2.rectangle(background_image, (x1, y1), (x2, y2), (0, 0, 255), 1)

    for pos_bbox in pos_bboxes:
        x1, y1, x2, y2 = pos_bbox
        cv2.rectangle(background_image, (x1, y1), (x2, y2), (0, 255, 0), 1)

    if pos_pred_bboxes is not None:
        for pos_pred_bbox in pos_pred_bboxes:
            x1, y1, x2, y2 = pos_pred_bbox
            cv2.rectangle(background_image, (x1, y1), (x2, y2), (255, 0, 0), 1)

    image_x, image_y = image_shape[:2]
    ax.imshow(background_image, extent=[0, image_x, 0, image_y])
    plt.show()


# assume our anchor_head network predicted delta
def predict_anchor(shape):
    delta_pred = torch.randn([9216, 4])
    return delta_pred
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;base_anchor = gen_base_anchors(32, torch.Tensor([0.5, 1, 2]), torch.Tensor([2]))
board = np.zeros((256, 256, 3))
all_anchors, shifts = grid_anchors(base_anchor, (32, 32), 16)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from math import sqrt
print(32/sqrt(2), 32 * sqrt(2))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;22.62741699796952 45.254833995939045
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for anchor in base_anchor:
    x1, y1, x2, y2 = np.array(anchor) + 112
    cv2.rectangle(board, (x1, y1), (x2, y2), (0, 255, 0), 1)

board[board&amp;gt;255] = 255
fig = plt.figure(figsize=(20,20))
ax = prepare_base_figure((1,1,1), 16)

ax.annotate(&#39;base anchor center at (0, 0)&#39;, xy=(0, 0), xytext=(0, -108), color=&#39;white&#39;,size=30,
            arrowprops=dict(facecolor=&#39;white&#39;, shrink=5),
            )

ax.annotate(&#39;-23, -45&#39;, xy=(-23, -45), xytext=(-48, -78), color=&#39;white&#39;,size=30,
            arrowprops=dict(facecolor=&#39;white&#39;, shrink=5),
            )
ax.annotate(&#39;-32, -32&#39;, xy=(-32, -32), xytext=(-68, -68), color=&#39;white&#39;,size=30,
            arrowprops=dict(facecolor=&#39;white&#39;, shrink=5),
            )
ax.annotate(&#39;-45, -23&#39;, xy=(-45, -23), xytext=(-78, -58), color=&#39;white&#39;,size=30,
            arrowprops=dict(facecolor=&#39;white&#39;, shrink=5),
            )

ax.annotate(&#39;23, 45&#39;, xy=(23, 45), xytext=(22, 72), color=&#39;white&#39;,size=30,
            arrowprops=dict(facecolor=&#39;white&#39;, shrink=5),
            )
ax.annotate(&#39;32, 32&#39;, xy=(32, 32), xytext=(43, 43), color=&#39;white&#39;,size=30,
            arrowprops=dict(facecolor=&#39;white&#39;, shrink=5),
            )
ax.annotate(&#39;45, 23&#39;, xy=(45, 23), xytext=(72, 22), color=&#39;white&#39;,size=30,
            arrowprops=dict(facecolor=&#39;white&#39;, shrink=5),
            )

ax.imshow(board, extent=[-128, 128, -128, 128])
&lt;/code&gt;&lt;/pre&gt;




  

&lt;figure&gt;

&lt;img src=&#34;/img/draw_anchor_7_3.png&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;A Base Anchor&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;base_anchor = gen_base_anchors(32, torch.Tensor([0.5, 1, 2]), torch.Tensor([2, 2*2**(1/3), 2*2**(2/3)]))
board = np.zeros((512, 512, 3))
for shift in shifts.cpu():
    shift = np.array(shift, dtype=np.uint16)
    #board[shift[1], shift[0]] = (255, 255, 0)
    cv2.circle(board, (shift[3], shift[2]), 2, (0,255,0))
ax = prepare_base_figure((1,1,1), 16)
ax.imshow(board, extent=[0, 512, 0, 512])
&lt;/code&gt;&lt;/pre&gt;




  

&lt;figure&gt;

&lt;img src=&#34;/img/draw_anchor_8_2.png&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;An Anchor Gird&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;base_anchor = gen_base_anchors(32, torch.Tensor([0.5, 1, 2]), torch.Tensor([2*2**(1/3)]))
board = np.zeros((512, 512, 3))
ax = prepare_base_figure((1,1,1), 16)

for shift in shifts.cpu():
    shift = np.array(shift, dtype=np.uint16)
    board[shift[1], shift[0]] = 255
    board[shift[3], shift[2]] = 255
for anchor in base_anchor:
    x1, y1, x2, y2 = np.array(anchor)
    cv2.rectangle(board, (x1, y1), (x2, y2), (0, 255, 0), 1)
    x1, y1, x2, y2 = np.array(anchor) + 128
    cv2.rectangle(board, (x1, y1), (x2, y2), (0, 0, 255), 1)
board[board&amp;gt;255] = 255

show_anchor = np.array(base_anchor[2])
moved_show_anchor = np.array(base_anchor[2]) + 128


ax.annotate(&#39;&#39;, xy=(144, 144), xytext=(16, 16), color=&#39;white&#39;,size=30,
            arrowprops=dict(facecolor=&#39;red&#39;, shrink=5),
            )
ax.annotate(&#39;shift size = 128, 128&#39;, xy=(16, 128), xytext=(15, 100), color=&#39;red&#39;,size=30
            )
ax.annotate(&#39;shifted anchor = 144,&#39;, xy=(144, 144), xytext=(256, 64), color=&#39;blue&#39;,size=30,
            arrowprops=dict(facecolor=&#39;white&#39;, shrink=5),
            )
ax.annotate(&#39;base anchor&#39;, xy=(16, 16), xytext=(128, 16), color=&#39;green&#39;,size=30,
            arrowprops=dict(facecolor=&#39;white&#39;, shrink=5),
            )
ax.annotate(&#39;172, 200&#39;, xy=(moved_show_anchor[3], moved_show_anchor[2]), xytext=(256, 160), color=&#39;white&#39;,size=30,
            arrowprops=dict(facecolor=&#39;white&#39;, shrink=5),
            )
ax.annotate(&#39;44, 72&#39;, xy=(show_anchor[3], show_anchor[2]), xytext=(100, 64), color=&#39;white&#39;,size=30,
            arrowprops=dict(facecolor=&#39;white&#39;, shrink=5),
            )
ax.imshow(board, extent=[0, 512, 512, 0])
&lt;/code&gt;&lt;/pre&gt;




  

&lt;figure&gt;

&lt;img src=&#34;/img/draw_anchor_9_2.png&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;How an Anchor shifts&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Anchor</title>
      <link>/post/anchor/</link>
      <pubDate>Wed, 01 May 2019 21:50:26 +0900</pubDate>
      
      <guid>/post/anchor/</guid>
      <description>

&lt;h1 id=&#34;anchor&#34;&gt;Anchor&lt;/h1&gt;

&lt;p&gt;SOTA 성능을 내는 많은 detector는 anchor를 활용해서 bbox coordinate을 학습합니다. anchor가 무엇인지, 어떻게 one-stage, two-stage detector에서 사용되는지 알아보겠습니다.&lt;/p&gt;

&lt;h2 id=&#34;what-is-anchor&#34;&gt;What is Anchor?&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Firstly suggested paper: Faster-RCNN?&lt;/li&gt;
&lt;li&gt;Papers using anchors

&lt;ul&gt;
&lt;li&gt;one-stage

&lt;ul&gt;
&lt;li&gt;RetinaNet&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;two-stage

&lt;ul&gt;
&lt;li&gt;Faster-RCNN&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Purpose:

&lt;ul&gt;
&lt;li&gt;image에 object가 있는 영역을 box로 예측해야 하는데, 예측을 용이하게 해주기 위해서 image로부터 얻은 feature map의 위치마다 default로 box를 여러 개를 그려서(anchor) 이 anchor들의 크기를 기준으로 차이에 대해서 학습하게 합니다. 즉, anchor의 크기가 적절하지 못한 경우에는 차이의 편차가 커지게 될 것이므로 학습이 어려워질 수 있어서 적절한 크기를 선정하는게 중요합니다.&lt;/li&gt;
&lt;li&gt;균일한 간격, 일정한 규칙으로 anchor를 생성하므로, 물체가 특정 위치에 존재할 때만 탐지가 잘 되거나, 혹은 특정 위치에서는 탐지가 잘 되지 않는 현상이 줄어듭니다. 이를 translation-Invariance라고 합니다.[1]&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Parameters:

&lt;ul&gt;
&lt;li&gt;scale: anchor size in &lt;strong&gt;feature map&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;ratio: anchor ratio in &lt;strong&gt;feature map&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;scale and ratio makes &lt;strong&gt;base anchor size&lt;/strong&gt; in &lt;strong&gt;feature map&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;real anchor sizes = base anchor size * stride&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;stride: stride * feature map pixel location = absolute center point of original image&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;how-to-draw-grid-anchors-anchor-generator&#34;&gt;How to draw grid anchors(&lt;code&gt;anchor_generator&lt;/code&gt;)&lt;/h2&gt;

&lt;p&gt;anchor를 그리기 위해서는 위 parameter들이 필요합니다. box를 예측할 때, 우리는 feature map의 pixel 단위로 예측하기 때문에 anchor도 feature map과 같은 width, height를 가지면 됩니다.&lt;/p&gt;

&lt;p&gt;stride는 &lt;code&gt;[image_width // anchor_width], [image_height // anchor_height]&lt;/code&gt;로 지정하는 경우에 image와 feature map 비율만큼의 크기를 anchor의 1개 pixel이 가지게 됩니다. 즉, image에서 상상을 하면 stride만큼 띄어서 anchor가 존재한다고 생각하시면 됩니다.(&lt;code&gt;grid_anchors&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;중심 좌표가 stride 만큼 떨어져서 존재한다고 보면 되고, 그 위에 그려지는 box의 크기는 base_anchor_size(&lt;code&gt;AnchorGenerator.base_anchors&lt;/code&gt;)가 결정하게 됩니다. scale, ratio 2개 parameter로 결정되는 크기이고 크기의 단위는 1stride가 됩니다. [2]&lt;/p&gt;

&lt;p&gt;RetinaNet의 경우 Octave scale을 사용하였습니다. faster-rcnn에서 사용한, 2,4,6 등 n배로 올라가는 scale 간격 대신 $2^{0}, 2^{\frac 1 3}, 2^{\frac 2 3}$과 같이 ${base scale}^{octave scale}$을 사용하였습니다. [3]&lt;/p&gt;

&lt;p&gt;feature map이 작은 경우, stride가 커지게 되고 scale, ratio의 image에서 실제 크기는 stride에 의해 결정되기 때문에 anchor box의 크기도 매우 커져서 예측하려는 물체가 상당히 클 것입니다.&lt;/p&gt;

&lt;p&gt;반대로 feature map이 큰 경우는 stride가 작고 위와 반대로 anchor box의 크기가 작아져서 예측하려는 물체가 작을 것입니다.&lt;/p&gt;

&lt;p&gt;이는 feature map의 크기에 따라서 예측하는 물체의 크기와도 상관이 있습니다.(보통 큰 feature map이 high-level 정보를 가지고 있어서 큰 물체를 예측 잘 하고, 작은 feature가 low-level 정보를 다뤄서 작은 물체 예측을 잘 한다고 알려져 있습니다.)&lt;/p&gt;

&lt;h2 id=&#34;anchor-as-a-target-anchor-target&#34;&gt;Anchor as a target (&lt;code&gt;anchor_target&lt;/code&gt;)&lt;/h2&gt;

&lt;p&gt;anchor는 학습할 때 box의 기본 틀로 사용된다고 했습니다. 위에서 anchor를 grid에 그리는 것을 완료했으면, target으로 변환해주는 과정을 거쳐야 합니다.&lt;/p&gt;

&lt;p&gt;학습 목표가 되는 target의 값은 anchor와 ground truth의 차이로 이루어지기 때문에 (&lt;strong&gt;delta 수식 추가&lt;/strong&gt;) 각 anchor와 ground truth 간의 overlap이 어느 정도 생기는 지(Intersect of Union)를 계산하고, 일정 IoU 이상 겹치는 경우와, 특정 ground truth와의 iou가 가장 높은 경우 positive label을 주고 그 anchor의 부분만 차이에 해당하는 delta를 계산해야 gt가 있는 anchor에 대해서만 실제 학습할 수 있게 됩니다.
또한 일정 iou 미만을 갖는 경우 negative label을 주고, classification loss에 사용하게 됩니다. [4]&lt;/p&gt;

&lt;h2 id=&#34;train-anchor&#34;&gt;Train anchor&lt;/h2&gt;

&lt;p&gt;anchor target을 만들었다면 앞에서 나온 feature를 network(&lt;code&gt;anchor_head&lt;/code&gt;)를 통과시켜 reg_pred로 delta를 예측하도록, score로 class를 예측하도록 학습시키면 됩니다.&lt;/p&gt;

&lt;p&gt;loss는 one/two-stage network 마다 다르게 적용되나 공통적으로 regression은 smooth-l1를, classification은 cross entropy를 가장 많이 사용합니다.
loss 계산에는 positive, negative sample을 모두 다 사용할 수는 있지만, positive sample에 비해 negative sample의 갯수가 압도적으로 많으므로, 일부 정해진 숫자 만큼만의 sample을 선정하여 학습에 사용합니다. positive:negative=1:1 [5]&lt;/p&gt;

&lt;h2 id=&#34;test&#34;&gt;Test&lt;/h2&gt;

&lt;p&gt;anchor에 대해서 bbox 예측을 delta로 하기 때문에, delta를 bbox로 변환해주는 과정이 필요합니다.&lt;/p&gt;

&lt;p&gt;delta는 &lt;strong&gt;anchor에 대한 차이&lt;/strong&gt;이기 때문에 anchor grid를 가지고 있으면 재변환해주는 과정은 어렵지 않습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;[1] Translation-Invariant Anchors
 An important property of our approach is that it is translation invariant, both in terms of the anchors and the functions that compute proposals relative to the anchors.
 If one translates an object in an image, the proposal should translate and the same function should be able to predict the proposal in either location.&lt;/li&gt;
&lt;li&gt;[2] Multi-Scale Anchors as Regression References
 Our design of anchors presents a novel scheme for addressing multiple scales (and aspect ratios).
 The second way is to use sliding windows of multiple scales (and/or aspect ratios) on the feature maps.&lt;/li&gt;
&lt;li&gt;[3] For denser scale coverage than in Faster-RCNN, at each level we add anchors of sizes {2^0,2^&lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt;, 2^&lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt;} of the original set of 3 aspect ratio anchors.
 This improve AP in our setting. In total there are A = 9 anchors per level and across levels they cover the scale range 32 - 813 pixels with respect to the network’s input image.&lt;/li&gt;

&lt;li&gt;&lt;p&gt;[4] For training RPNs, we assign a binary class label (of being an object or not) to each anchor.
 We assign a positive label to two kinds of anchors:
 (i) the anchor/anchors with the highest Intersection-overUnion (IoU) overlap with a ground-truth box, or
 (ii) an anchor that has an IoU overlap higher than 0.7 with any ground-truth box.
 Note that a single ground-truth box may assign positive labels to multiple anchors.
 We assign a negative label to a non-positive anchor if its IoU ratio is lower than 0.3 for all ground-truth boxes.
 Anchors that are neither positive nor negative do not contribute to the training objective.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;[5] It is possible to optimize for the loss functions of all anchors, but this will bias towards negative samples as they are dominate.
 Instead, we randomly sample 256 anchors in an image to compute the loss function of a mini-batch, where the sampled positive and negative anchors have a ratio of up to 1:1.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
