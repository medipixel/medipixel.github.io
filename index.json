[{"authors":["young-kim","whi-kwon"],"categories":[],"content":" Introduction SOTA 성능을 내는 많은 detector에서 anchor를 활용해서 bbox coordinate을 학습합니다. anchor가 무엇인지, 어떻게 one-stage, two-stage detector에서 사용되는지 살펴보겠습니다.\nImport Libraries 코드는 mmdet.v0.6rc0을 기준으로 참고하여 제작하였습니다.\n%load_ext autoreload %autoreload 2 import cv2 import numpy as np import matplotlib.pyplot as plt import matplotlib.ticker as plticker import torch import torch.nn.functional as F from matplotlib.lines import Line2D from matplotlib.patches import Patch  from anchor_generator import (gen_base_anchors, get_anchors, grid_anchors, meshgrid) from assigner import assign_wrt_overlaps, bbox_overlaps from loss import binary_cross_entropy, smooth_l1_loss from prediction import predict_anchors from transforms import bbox2delta, delta2bbox from visualize import (draw_anchor_gt_overlaps, draw_anchor_samples_on_image, draw_base_anchor_on_grid, draw_pos_assigned_bboxes)  What is Anchor?  첫 제안: Anchor라는 개념은 Faster R-CNN에서 처음으로 제안되었습니다. 주요 모델: Anchor는 대부분의 one-stage, two-stage detector에서 사용하며 대표적으로는 RetinaNet(one-stage)와 Faster R-CNN(two-stage)가 존재합니다. [1] 목적:  Object detection은 이미지 상에 object가 있는 영역을 bounding box(bbox)로 예측해야 합니다. 이런 예측을 용이하게 해주기 위해서 이미지로부터 얻은 feature map의 매 pixel 위치마다 bbox를 여러 개를 그립니다.(anchor) 이 anchor들과 gt를 비교하고 겹치는 영역을 기준으로 학습 대상으로 사용할 anchor를 선별합니다. 선별한 anchor를 이용해서 anchor와 정답(ground-truth)과의 차이에 대해서 학습합니다.(이 때, anchor의 크기가 적절하지 못한 경우에는 차이의 편차가 커지게 될 것이므로 학습이 어려워질 수 있어서 적절한 크기를 선정하는게 중요합니다.) anchor는 균일한 간격, 일정한 규칙으로 생성되어, 물체가 특정 위치에 존재할 때만 탐지가 잘 되거나, 혹은 특정 위치에서는 탐지가 잘 되지 않는 현상을 줄입니다. 이를 translation-Invariance라고 합니다. [2]  Parameters:  scale: feature map에서의 anchor 크기(scale)입니다. ratio: feature map에서의 anchor 비율(ratio)입니다. stride: image를 기준으로 어느 정도 간격으로 anchor를 생성할 것인지 나타내는 값입니다.(주로 image와 feature map 크기의 비율 값을 사용합니다.)  scale과 ratio가 feature map 내에서의 base_anchor_size를 만들게 됩니다. feature map의 크기는 image의 너비, 높이를 stride로 나눈 값이기 때문에 이게 반영된 image에서의 anchor 크기는 base_anchor_size * stride 입니다.    How to draw grid anchors  1개 anchor bbox의 coordination은 [x1, y1, x2, y2]로 표현할 수 있습니다. anchor는 feature map의 예측 값에 매칭되어야 하기 때문에 feature map과 동일한 width, height를 가지며 channel은 4로 갖습니다. base_anchor는 기본적인 anchor의 모습입니다. feature map과 동일한 width, height를 갖더라도 실제 이미지 상 크기에서 anchor가 어디에 위치하는지를 알 수 있어야 합니다. 그래서 stride를 고려합니다.  stride를 [image_width // feature_map_width] == [image_height // feature_map_height]로 지정하는 경우에 image와 feature map 비율만큼의 크기를 anchor의 1개 pixel이 가지게 됩니다. 즉, image에서 생각을 하면 stride만큼 띄어서 anchor가 존재한다고 생각하시면 됩니다.(grid_anchors) 중심 좌표가 stride 만큼 떨어져서 존재한다고 보면 되고, 그 위에 그려지는 bbox의 크기는 base_anchor_size(AnchorGenerator.base_anchors)가 결정하게 됩니다. scale, ratio 2개 parameter로 결정되는 크기이고 크기의 단위는 1 stride가 됩니다. [3]  RetinaNet의 경우 Octave scale을 사용하였습니다. Faster R-CNN에서 사용한, $2,4,6$ 등 $n$배로 올라가는 scale 간격 대신 $2^0, 2^{\\frac 1 3}, 2^{\\frac 2 3}$과 같이 (base scale)^(octave scale)을 사용하였습니다. [4] base_anchor_size는 scale, ratio에 의해 결정되어 feature map에 동일하게 적용됩니다. 하지만, feature map이 작은 경우, stride가 커지게 되고 반대의 경우엔 stride가 작아지게 되어 image에서의 anchor bbox 크기는 feature map의 영향을 받습니다.  anchor box가 크다는 건, 큰 물체를 잡는데 유리할 것이고 anchor box가 작은 경우엔 작은 물체를 잡는데 유리할 것입니다. 이는 feature map의 크기에 따라서 예측하는 물체의 크기와도 상관이 있습니다.(보통 CNN에서의 큰 feature map이 high-level 정보를 가지고 있어서 큰 물체를 예측 잘 하고, 작은 feature가 low-level 정보를 다뤄서 작은 물체 예측을 잘 한다고 알려져 있습니다.)   Settings anchor를 그리기 위한 hyperparameter들을 설정하겠습니다.\n gt_bboxes_list 의 bbox 크기를 크게 잡으면 다양한 positive anchor 후보들이 생기는 것을 확인할 수 있습니다. scales, ratios를 조절해서 anchor bbox의 형태를 편향되게 만들 수 있습니다. base_size는 (image_size // featmap_size) == anchor_stride의 값을 주로 갖는데, 이보다 크거나 작으면 전체 이미지를 커버하지 못하거나 이미지를 넘어서 커버하게 될 수 있습니다. 이 자료에서는 base_size와 anchor_stride의 값을 같게 놓겠습니다. 1개 feature map에 대해서만 anchor 분석을 진행하겠습니다. multi-level feature map(FPN 등)이 사용되는 경우는 후속 자료에서 살펴보도록 하겠습니다.  base_size = anchor_stride = 32 scales = torch.Tensor([2, 4, 8]) ratios = torch.Tensor([0.5, 1.0, 2.0]) featmap_size = [16, 16] device = 'cpu' image_shape = [256, 256, 3] anchors_per_grid = len(scales) * len(ratios) # x1y1x2y2 gt_bboxes_list = torch.FloatTensor([[32, 32, 32*3, 32*3]]).to(device)  Base Anchor base anchor를 원점이 중심인 좌표계에 그려봅니다. 이 base anchor는 scales * ratios의 개수만큼 생성되며 feature map 각 pixel의 해당 위치에 존재하게 됩니다.\nbase_anchor = gen_base_anchors(base_size, ratios, scales[:1]) draw_base_anchor_on_grid(base_anchor, base_size)  Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).     각 feature map의 pixel은 원 image 기준 좌표가 있을 것입니다. 이 좌표들에 base anchor를 더해주면 feature map 기준 각 pixel에 base anchor가 존재하게 되고 image 기준으로 stride 만큼 띄엄띄엄 base anchor가 존재하는 것으로 해석할 수 있습니다.  feature map 모든 pixel에 할당된 anchor와 이 때 anchor들 간 거리인 shifts를 구하고 몇 개의 샘플을 시각화해봅시다.   draw_anchor_samples_on_image(image_shape, base_size, featmap_size, scales, ratios)     get_anchors라는 함수를 통해 전체 anchor와 각각에 대한 valid 여부를 나타내는 flag를 얻습니다.  flag은 anchor가 이미지를 벗어나거나 할 때 학습에 사용하지 않기 위한 flag 입니다.   anchors, flags = get_anchors(image_shape, featmap_size, base_size, anchor_stride, scales, ratios, device)  assert anchors.shape[0] == featmap_size[0] * featmap_size[1] * 9 # feature map 32x32 각 pixel에 9개의 anchors assert len(flags) == len(anchors) # anchor를 사용할 지 말지 결정하는 flags와 anchors의 개수는 같아야 합니다.  Anchor Selection anchor는 gt와의 overlap 정도에 따라서 positive, negative를 배정합니다. 이 배정된 값들은 이 후에 regression, classification에 활용되게 됩니다.\n positive는 classification, regression 모두에 활용됩니다. 그래야 특정 bbox에 대해서 object의 class와 영역을 예측할 수 있게 됩니다. negative는 classification에만 활용됩니다. 그 이유는 negative의 경우 background라는 정보는 가지고 있지만, 어느 위치에 물체가 있다는 정보는 가지고 있지 않기 때문입니다. [5] overlap은 IoU(Intersection over Union)를 통해 계산합니다.  overlaps = bbox_overlaps(gt_bboxes_list, anchors)  assert overlaps.shape == (len(gt_bboxes_list), anchors.shape[0])  draw_anchor_gt_overlaps(overlaps, gt_bboxes_list, featmap_size, anchors_per_grid, anchor_stride=anchor_stride, draw_gt=True)    draw_anchor_gt_overlaps(overlaps, gt_bboxes_list, featmap_size, anchors_per_grid, anchor_stride)    # gt와의 overlap에 따라 pos, negative를 배정합니다. num_gts, assigned_gt_inds, max_overlaps = assign_wrt_overlaps(overlaps)  pos_inds = torch.nonzero(assigned_gt_inds \u0026gt; 0).squeeze(-1).unique() # positive indices neg_inds = torch.nonzero(assigned_gt_inds == 0).squeeze(-1).unique() # negative indices  # positive와 1:1 비율로 학습에 사용할 negative sample을 얻습니다. sampled_neg_inds = neg_inds[torch.randint(0, len(neg_inds), size=(len(pos_inds), ))]  pos_neg_cls_label = torch.cat([torch.ones(len(pos_inds)), torch.zeros(len(sampled_neg_inds))])  bboxes = anchors # bboxes pos_bboxes = bboxes[pos_inds] # positive boxes pos_assigned_gt_inds = assigned_gt_inds[pos_inds] - 1 pos_gt_bboxes = gt_bboxes_list[pos_assigned_gt_inds, :]  Anchor as a Target gt-anchor 차이에 대해서 학습해야 하기 때문에 [6] anchor bbox를 coordination([x1, y1, x2, y2]) 형태에서 target(target_delta)으로 변환해주는 과정을 거쳐야 합니다.\n# target_deltas는 특정 pos_inds에 대한 것이며 이 inds에 할당된 anchor를 기준으로만 loss가 계산이 되도록 해야 합니다. target_deltas = bbox2delta(pos_bboxes, pos_gt_bboxes)  # 변환이 제대로 되었는지 확인합니다. bboxes_reversed = delta2bbox(pos_bboxes, target_deltas) assert torch.equal(bboxes_reversed[0], gt_bboxes_list[0])  Train anchor anchor target을 만들었다면 앞에서 나온 feature를 network(anchor_head)를 통과시켜 regression 예측 값(reg_pred)으로 delta를 예측하도록, class 예측 값(score)으로 실제 class를 예측하도록 학습시킵니다.\n loss는 one/two-stage network 마다 다르게 적용되나 공통적으로 regression은 smooth-l1를, classification은 cross entropy를 가장 많이 사용합니다. loss 계산에는 positive, negative sample을 모두 다 사용할 수는 있지만, positive sample에 비해 negative sample의 개수가 압도적으로 많으므로, 일부 정해진 숫자 만큼만의 sample을 선정하여 학습에 사용합니다.(e.g. positive:negative=1:1.) [7]\n 아래에서 예측 값을 구하는 과정에서 엄밀하게는 anchor prediction을 구하고 그 중에 pos_inds에 해당하는 값만 가져오는 과정을 거쳐야 하는데 편의를 위해서 해당 과정을 거쳐서 pos_delta_pred를 구했다고 하겠습니다.\n loss를 구한 뒤에 gradient descent 하는 과정은 생략하겠습니다.\n  pos_neg_cls_pred, pos_delta_pred = predict_anchors(anchors.shape, target_deltas, sampled_neg_inds)  # regression, class loss를 각각 계산합니다. reg_loss = smooth_l1_loss(pos_delta_pred, target_deltas, beta=1.0) print(\u0026quot;reg_loss:\u0026quot;, reg_loss) cls_loss = binary_cross_entropy(pos_neg_cls_pred, pos_neg_cls_label) print(\u0026quot;cls_loss:\u0026quot;, cls_loss)  reg_loss: tensor(0.0795) cls_loss: tensor(2.7997)  Test  feature map을 받아 bbox의 cls_pred, reg_pred를 예측할 때 reg_pred를 delta로 하기 때문에, delta를 bbox로 변환해주는 과정이 필요합니다.(delta2bbox) delta는 gt-anchor의 차이이기 때문에 anchor bbox의 coordination 정보를 가지고 있으면 재변환해주는 과정은 수식적으로 풀면 되어 어렵지 않습니다. 최종적으로 object 예측 결과는 cls_pred가 특정 threshold 이상인 값들에 대해서 Non-maximum suppresion(NMS)를 통과시킨 결과입니다.\n cls_pred threshold, nms가 모두 고려되었다고 가정하고 위에서 얻은 pos_delta_pred를 test 결과로 얻었다고 가정하겠습니다.\n  pos_bboxes_pred = delta2bbox(pos_bboxes, pos_delta_pred)  아래 그림에서는 positve prediction들에 대해서 예측한 값을 순서대로 나타내었습니다.\n# blue: gt, green: anchor, red: prediction32, 32, 9 draw_pos_assigned_bboxes(image_shape, base_size, gt_bboxes_list, pos_bboxes, pos_bboxes_pred)      Reference Faster R-CNN arXiv:1506.01497[cs.CV]\n [2] Translation-Invariant Anchors An important property of our approach is that it is translation invariant, both in terms of the anchors and the functions that compute proposals relative to the anchors. If one translates an object in an image, the proposal should translate and the same function should be able to predict the proposal in either location.\n [3] Multi-Scale Anchors as Regression References Our design of anchors presents a novel scheme for addressing multiple scales (and aspect ratios). The second way is to use sliding windows of multiple scales (and/or aspect ratios) on the feature maps.\n [5] For training RPNs, we assign a binary class label (of being an object or not) to each anchor. We assign a positive label to two kinds of anchors: (i) the anchor/anchors with the highest Intersection-overUnion (IoU) overlap with a ground-truth box, or (ii) an anchor that has an IoU overlap higher than $0.7$ with any ground-truth box. Note that a single ground-truth box may assign positive labels to multiple anchors. We assign a negative label to a non-positive anchor if its IoU ratio is lower than $0.3$ for all ground-truth boxes. Anchors that are neither positive nor negative do not contribute to the training objective.\n [6] For bounding box regression, we adopt the parameterizations of the 4 coordinates following: $$ t_x = (x - x_a) / w_a,\\ t_y = (y - y_a) / h_a, t_w = \\log(w / w_a),\\ t_h = \\log(h / h_a), t_x^{\\ast} = (x^{\\ast} - x_a) / w_a,\\ t_y^{\\ast} = (y^{\\ast} - y_a) / h_a, t_w^{\\ast} = \\log(w^{\\ast} / w_a),\\ t_h^{\\ast} = \\log(h^{\\ast} / h_a) $$\n [7] It is possible to optimize for the loss functions of all anchors, but this will bias towards negative samples as they are dominate. Instead, we randomly sample $256$ anchors in an image to compute the loss function of a mini-batch, where the sampled positive and negative anchors have a ratio of up to $1:1$.\nRetinaNet arXiv:1708.02002[cs.CV]\n [1] The design of our RetinaNet detector shares many similarities with previous dense detectors, in particular the concept of \u0026lsquo;anchors\u0026rsquo; introduced by RPN and use of features pyramids as in SSD and FPN.\n [4] We use translation-invariant anchor boxes similar to those in the RPN variant. The anchors have areas of $32^2$ to $512^2$ on pyramid levels $P_3$ to $P_7$, respectively. at each pyramid level we use anchors at three aspect ratios ${1:2, 1:1, 2:1}$. For denser scale coverage, at each level we add anchors of sizes ${2^0,2^{\\frac 1 3}, 2^{\\frac 2 3}}$ of the original set of 3 aspect ratio anchors. This improve AP in our setting. In total there are $A=9$ anchors per level and across levels they cover the scale range $32-813$ pixels with respect to the network\u0026rsquo;s input image. Each anchor is assigned a length $K$ one-hot vector of classification targets, where $K$ is the number of object classes, and a $4$-vector of box regression targets. We use the assignment rule from RPN but modified for multi-class detection and with adjusted thresholds. Specifically, anchors are assigned to ground-truth object boxes using an intersection-over-union(IoU) threshold of $0.5$; and to background if their IoU is in $[0, 0.4)$. As each anchor is assigned to at most one object box, we set the corresponding entry in its length $K$ label vector to $1$ and all other entries to $0$. If an anchor is unassigned, which may happen with overlap in $[0.4, 0.5)$, it is ignored during training. Box regression targets are computed as the offset between each anchor and its assigned object box, or omitted if there is no assignment.\n[8] The classification subnet predicts the probability of object presence at each spatial position for each of the $A$ anchors and $K$ object classes.\n[9] In parallel with the object classification subnet, we attach another small FCN to each pyramid level for the purpose of regressing the offset from each anchor box to a nearby ground-truth object, if one exists. For each of the $A$ anchors per spatial location, these $4$ outputs predict the relative offset between the anchor and the ground-truth box.\n","date":1559189700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559189700,"objectID":"ad6a46b17ba7d0b3094947e24dc6e00a","permalink":"/post/anchor-target/","publishdate":"2019-05-30T13:15:00+09:00","relpermalink":"/post/anchor-target/","section":"post","summary":"Anchor algorithm in MMDetection","tags":[],"title":"Anchor","type":"post"},{"authors":["young-kim","whi-kwon"],"categories":[],"content":" Import Libraries import os import sys import math from platform import python_version import cv2 import matplotlib import matplotlib.pyplot as plt import numpy as np print(\u0026quot;Python version : \u0026quot;, python_version()) print(\u0026quot;Opencv version : \u0026quot;, cv2.__version__) matplotlib.rcParams['figure.figsize'] = (4.0, 4.0)  Python version : 3.6.6 Opencv version : 3.4.3  Data load sample_image_path = '../image/' sample_image = 'lena_gray.jpg' img = cv2.imread(sample_image_path + sample_image, cv2.IMREAD_GRAYSCALE)  Data description  본 예제에서 사용할 데이터는 아래와 같습니다.  거의 대부분의 OpenCV 예제에서 볼 수 있는 Lena 입니다. 단순한 특징의 배경과 복잡한 특징의 인물이 함께 존재하여 다양한 condition을 test하기 좋은 data로 널리 알려져 있습니다.   plt.imshow(img, cmap='gray') plt.title('Lena') plt.show()    Histogram  Histogram이란, 이미지에서 특정 픽셀값의 등장 빈도를 전체 픽셀 갯수 대비 비율로 나타낸 그래프 입니다. 이미지의 전체적인 명암 분포를 한 눈에 확인할 수 있습니다. 두 가지 예제 코드를 통해 Histogram에 대해 알아보겠습니다.  def plot_histogram_npy(img): w, h = img.shape w, h = int(w), int(h) hist_cnt = np.zeros(255) hist = np.zeros(255) for j in range(h): for i in range(w): hist_cnt[img[j, i]] += 1 hist = hist_cnt / (w * h) plt.plot(hist) plt.title('Histogram of Lena, numpy', size=15)  plot_histogram_npy(img)     OpenCV등을 이용하지 않고 numpy만을 이용하여 구한 Lena의 Histogram 입니다. 이미지에서 개별 픽셀값이 몇 번씩 등장하는지 확인하고 전체 픽셀 수로 normalize하여 Histogram을 얻게 됩니다.  def plot_histogram_cv(img): w, h = img.shape hist = cv2.calcHist([img], [0], None, [256], [0, 256]) hist_norm = hist / (w * h) # normalize plt.plot(hist_norm) plt.title('Histogram of Lena, Opencv', size=15)  plot_histogram_cv(img)     OpenCV를 이용하면 함수 호출을 통해 간단하게 Histogram을 구할 수 있습니다. numpy로 구한 Histogram과 비교해 보면, 두 결과물이 완전히 동일한 것을 알 수 있습니다. \u0026lsquo;cv2.calcHist()\u0026rsquo;를 수행하면 픽셀값 별 등장 횟수의 그래프를 얻고, 이를 normalize하여 최종적으로 Histogram을 얻게 됩니다. \u0026lsquo;cv2.calcHist()\u0026rsquo;의 자세한 사용법은 Opencv 공식 tutorial page를 통해 확인할 수 있습니다. [2] \u0026mdash;  Histogram\u0026nbsp;Equalization  Histogram Equalization(히스토그램 평활화)란, pixel값 0부터 255까지의 누적치가 직선 형태가 되도록 만드는 이미지 처리 기법 입니다.  히스토그램 평활화 기법은 이미지가 전체적으로 골고루 어둡거나 골고루 밝아서 특징을 분간하기 어려울 때 자주 쓰입니다.  설명이 난해하니 코드를 통해 자세히 알아보겠습니다.  def show_stacked_histogram(img): stack_hist = np.zeros(255, dtype=np.float32) eq_hist = np.zeros(255, dtype=np.float32) w, h = img.shape hist = cv2.calcHist([img], [0], None, [256], [0, 256]) for i in range(255): stack_hist[i] = np.sum(hist[:i]) eq_hist[i] = round(stack_hist[i]) eq_hist /= (w * h) plt.plot(eq_hist) plt.title('Stacked Histogram', size=15) plt.show()  show_stacked_histogram(img)     0부터 255까지의 Histogram의 누적치 입니다. 쉽게 말하면 Histogram을 적분한 것이라고 할 수 있습니다.  e.g) eq_hist[150] = 0.675 \u0026rarr; 이미지 내에서 0부터 150까지의 pixel이 차지하는 비율 = 67.5% 당연히 항상 eq_hist[0] = 0이며, eq_hist[255] = 1.0 입니다.  전체적으로 직선에 가까운 형태지만 x좌표기준 0 근처와 255 근처는 수평인 것을 알 수 있습니다.  이 말은 Lena image에서 pixel 값 기준 0 근처와 255 근처가 존재하지 않는다는 말 입니다. 즉, 다시 말해 전체 pixel 값들에 대하여 분포가 균일하지 않다는 말 입니다.  히스토그램 평활화는 이와 같이 균일하지 않은 픽셀값의 분포를 고르게 만드는 작업입니다. \u0026mdash; 그럼 이제 히스토그램 평활화를 해보겠습니다.  equ = cv2.equalizeHist(img) show_stacked_histogram(equ)     OpenCV에 구현되어있는 cv2.equalizeHist()함수를 통해 평활화한 Lena의 Histogram입니다. 이제 Histogram 누적치가 직선 형태라는 말이 확실하게 이해 되실 것 같습니다.   마지막으로 이렇게 변화시킨 이미지가 원본 이미지와 어떻게 다른지 확인해 보겠습니다.  plt.figure(figsize=(8,8)) plt.subplot(121) plt.imshow(img, cmap='gray') plt.title('Original Lena') plt.subplot(122) plt.imshow(equ, cmap='gray') plt.title('Equalized Lena') plt.show()     전체적인 톤의 변화를 확인할 수 있는데, 밝은 부분은 더 밝아지고 어두운 부분은 더 어두워지는 모습을 볼 수 있습니다.  이는 원본 이미지가 중간 정도 밝기의 픽셀을 다수 포함하고 있었고, 상대적으로 아주 어둡거나 아주 밝은 부분은 적었기 때문입니다. 히스토그램 평활화를 통해 모든 픽셀값이 동일한 비율로 등장하게끔 수정하여 이와 같은 변화가 일어났다고 볼 수 있습니다.  히스토그램 평활화에 대한 자세한 내용은 마찬가지로 OpenCV 공식 tutorial page를 통해 확인할 수 있습니다. [3]  Reference  [1] 오일석, 컴퓨터 비전, 2014, pp. 58-63 [2] Alexander Mordvintsev \u0026amp; Abid K., \u0026lsquo;Histograms - 1 : Find, Plot, Analyze !!!\u0026lsquo;, OpenCV-python tutorials. 2013 [Online]. Available: https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_histograms/py_histogram_begins/py_histogram_begins.html#histogram-calculation-in-opencv [Accessed: 29- Mar- 2019] [3] Alexander Mordvintsev \u0026amp; Abid K., \u0026lsquo;Histograms - 2: Histogram Equalization\u0026rsquo;, OpenCV-python tutorials. 2013 [Online]. Available: https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_histograms/py_histogram_equalization/py_histogram_equalization.html#histogram-equalization [Accessed: 29- Mar- 2019]  ","date":1556716937,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556716937,"objectID":"5e65672a04f766afe65da92e59b7d863","permalink":"/post/histogram/","publishdate":"2019-05-01T22:22:17+09:00","relpermalink":"/post/histogram/","section":"post","summary":"Histogram using OpenCV","tags":[],"title":"2.2 Histogram","type":"post"},{"authors":["young-kim","whi-kwon"],"categories":[],"content":" Import Libraries import os import sys import math from platform import python_version import cv2 import matplotlib.pyplot as plt import matplotlib import numpy as np print(\u0026quot;Python version : \u0026quot;, python_version()) print(\u0026quot;Opencv version : \u0026quot;, cv2.__version__) matplotlib.rcParams['figure.figsize'] = (4.0, 4.0)  Python version : 3.6.7 Opencv version : 3.4.5  Data load sample_image_path = '../image/' sample_image = 'lena_gray.jpg' img = cv2.imread(sample_image_path + sample_image, cv2.IMREAD_GRAYSCALE) coin_image = 'coins.jpg' mask = np.array([[0, 1, 0],[1, 1, 1], [0, 1, 0]], dtype=np.uint8) coin_img = cv2.imread(sample_image_path + coin_image, cv2.IMREAD_GRAYSCALE) ret, coin = cv2.threshold(coin_img, 240, 255, cv2.THRESH_BINARY_INV) coin = cv2.dilate(coin, mask, iterations=1) coin = cv2.erode(coin, mask, iterations=6)  Data description  본 예제에서 사용할 데이터는 아래와 같습니다.  Lena : 지난 예제에서 사용한 Lena 입니다. Coins : Blob labeling 예제에서 사용될 이미지 입니다.  blob labeling이라는 주제에 맞게 전처리가 된 이미지 입니다. (blob labeling에서 자세하게 설명합니다.)    plt.subplot(1, 2, 1) plt.imshow(img, cmap='gray') plt.title('Lena') plt.subplot(1, 2, 2) plt.imshow(coin, cmap='gray') plt.title('Coins') plt.show()    Binarization  Binarization(이진화)이란, grayscale의 이미지를 기준에 따라 0 또는 1의 값만을 갖도록 만드는 작업입니다.  일반적으로 특정 픽셀 값을 기준으로 더 작은 값은 0으로, 더 큰 값은 1로 만듭니다. 사용 목적에 따라 결과 값을 반전시킬 때도 있는데, 추후에 코드 예시로 알아보겠습니다.  이진화 자체는 단순한 작업이나, 추후에 보다 발전된 알고리즘을 다루기 위한 기본이 됩니다.  def simple_img_binarization_npy(img, threshold): w, h = img.shape b_img = np.zeros([w, h]) b_img[img \u0026gt; threshold] = 1 return b_img  plt.figure(figsize=(8, 8)) b_img1 = simple_img_binarization_npy(img, 200) b_img2 = simple_img_binarization_npy(img, 150) b_img3 = simple_img_binarization_npy(img, 100) plt.subplot(2, 2, 1) plt.imshow(img, cmap='gray') plt.title('Gray Lena') plt.subplot(2, 2, 2) plt.imshow(b_img1, cmap='gray') plt.title('Lena over 200') plt.subplot(2, 2, 3) plt.imshow(b_img2, cmap='gray') plt.title('Lena over 150') plt.subplot(2, 2, 4) plt.imshow(b_img3, cmap='gray') plt.title('Lena over 100') plt.suptitle('Lena with different threshold value (numpy)', size=15) plt.show()     위에서 numpy를 이용했다면, 이번엔 OpenCV 내장 함수를 이용하여 image threshold를 해보겠습니다. OpenCV 함수로는 보다 편리하게 다양한 결과물을 만들어낼 수 있습니다.  cv2.threshold() 함수에 전달하는 인자를 통해 threshold 알고리즘을 다양하게 변경할 수 있습니다.   ret, thresh1 = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY) ret, thresh2 = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY_INV) ret, thresh3 = cv2.threshold(img, 127, 255, cv2.THRESH_TRUNC) ret, thresh4 = cv2.threshold(img, 127, 255, cv2.THRESH_TOZERO) ret, thresh5 = cv2.threshold(img, 127, 255, cv2.THRESH_TOZERO_INV) plt.figure(figsize=(8, 12)) plt.subplot(3, 2, 1) plt.imshow(thresh1, cmap='gray') plt.title('threshold lena') plt.subplot(3, 2, 2) plt.imshow(thresh2, cmap='gray') plt.title('inversed threshold lena') plt.subplot(3, 2, 3) plt.imshow(thresh3, cmap='gray') plt.title('truncated threshold lena') plt.subplot(3, 2, 4) plt.imshow(thresh4, cmap='gray') plt.title('zero threshold lena') plt.subplot(3, 2, 5) plt.imshow(thresh5, cmap='gray') plt.title('inversed zero threshold lena') plt.suptitle('Lena with different threshold algorithm(OpenCV)', size=15) plt.show()    Otsu\u0026nbsp;Algorithm  Otsu algorithm(오츄 알고리즘)은 특정 threshold값을 기준으로 영상을 둘로 나눴을때, 두 영역의 명암 분포를 가장 균일하게 할 때 결과가 가장 좋을 것이다는 가정 하에 만들어진 알고리즘입니다.  여기서 균일함 이란, 두 영역 각각의 픽셀값의 분산을 의미하며, 그 차이가 가장 적게 하는 threshold 값이 오츄 알고리즘이 찾고자 하는 값입니다.  위에 기술한 목적에 따라, 알고리즘에서는 특정 Threshold T를 기준으로 영상을 분할하였을 때, 양쪽 영상의 분산의 weighted sum이 가장 작게 하는 T값을 반복적으로 계산해가며 찾습니다.  weight는 각 영역의 넓이로 정합니다. 어떤 연산을 어떻게 반복하는지에 대한 내용이 아래 수식에 자세히 나와있습니다.   $T = argmin_{t\\subseteq {1,\\cdots,L-1}} v_{within}(t) \\\\\nv_{within}(t) = w_{0}(t)v_{0}(t) + w_{1}(t)v_{1}(t) \\\\\n\\begin{align} \u0026amp; w_{0}(t) = \\Sigma_{i=0}^{t} \\hat h(i),\\hspace{2cm} \u0026amp;\u0026amp; w_{1}(t) = \\Sigma_{i=t+1}^{L-1} \\hat h(i)\\\\\n\u0026amp; \\mu_{0}(t)=\\frac{1}{w_{0}(t)}\\Sigma_{i=0}^{t}i\\hat h(i) \u0026amp;\u0026amp; \\mu_{1}(t)=\\frac{1}{w_{1}(t)}\\Sigma_{i=t+1}^{L-1}i\\hat h(i)\\\\\n\u0026amp; v_{0}(t) = \\frac{1}{w_{0}(t)}\\Sigma_{i=0}^{t}i\\hat h(i)(i-\\mu_{0}(t))^2 \u0026amp;\u0026amp; v_{1}(t) = \\frac{1}{w_{1}(t)}\\Sigma_{i=t+1}^{L-1}i\\hat h(i)(i-\\mu_{1}(t))^2\\\\\n\\end{align} $\n $w_{0}(t), w_{1}(t)$는 threshold 값으로 결정된 흑색 영역과 백색 영역의 크기를 각각 나타냅니다. $v_{0}(t), v_{1}(t)$은 두 영역의 분산을 뜻합니다.\n 위 수식을 그대로 적용하면 시간복잡도가 $\\Theta(L^{2})$이므로 실제로 사용하기 매우 어려워집니다.\n 그러나, $\\mu와 v$가 영상에 대해 한번만 계산하고 나면 상수처럼 취급된다는 사실에 착안하여 다음 알고리즘이 완성되었습니다.\n  $ T = argmax_{t\\subseteq{0,1,\\cdots,L-1}}v_{between}(t)\\\\\nv_{between}(t)=w_{0}(t)(1-w_{0}(t))(\\mu_{0}(t)-\\mu_{1}(t))^2\\\\\n\\mu = \\Sigma_{i=0}^{L-1}i\\hat h(i)\\\\\n초깃값(t=0):w_{0}(0)=\\hat h(0), \\mu_{0}(0)=0\\\\\n순환식(t\u0026gt;0):\\\\\n\\begin{align} \\hspace{1cm}\u0026amp; w_{0}(t)=w_{0}(t-1)+\\hat h(t)\\\\\n\u0026amp; \\mu_{0}(t)=\\frac{w_{0}(t-1)\\mu_{0}(t-1)+t\\hat h(t)}{w_{0}(t)}\\\\\n\u0026amp; \\mu_{1}(t)=\\frac{\\mu-w_{0}(t)\\mu_{0}(t)}{1-w_{0}(t)}\\\\\n\\end{align} $\n 위 순환식을 t에 대하여 수행하여 가장 큰$v_{between}$를 갖도록 하는 $t$를 최종 threshold T로 사용합니다. 이와 같은 알고리즘이 OpenCV의 threshold함수에 구현되어있으며, \u0026lsquo;cv2.THRESH_OTSU\u0026rsquo; 파라미터를 사용하면 적용됩니다.아래와 같이 사용하면 됩니다.  plt.figure(figsize=(8, 4)) ret, th = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU) plt.subplot(1, 2, 1) plt.imshow(img, cmap='gray') plt.title('Gray Lena') plt.subplot(1, 2, 2) plt.imshow(th, cmap='gray') plt.title('Otsu Lena') plt.suptitle('Lena with Otsu threshold value', size=15) plt.show()    Blob\u0026nbsp;labeling  Threshold를 통해 할 수 있는 일은 그야말로 무궁무진한데, 그 중 하나로 이미지 분할(image segmentation)을 들 수 있습니다.\n 만일 threshold 등의 알고리즘을 이용하여 특정 목적에 따라 영상을 분할할 수 있다면(e.g. 사람 손 or 도로의 차선) 1로 정해진 픽셀끼리 하나의 object라고 생각할 수 있을것이고, 우리는 이 object를 묶어서 사용하고 싶게 될 것입니다.  서로 다른 object인지를 판단하기 위하여 픽셀의 연결성 [2] 을 고려한 알고리즘을 수행하고 각기 다른 label을 할당하는데, 이를 blob labeling이라 합니다.\n Blob labeling을 하면 개별 object에 대해 각각 접근하여 우리가 하고싶은 다양한 영상처리를 개별적으로 적용할 수 있게 되니, 활용도가 아주 높은 기능입니다.\n 본 예제에서는 blob labeling에 대한 개념적인 소개와 OpenCV에 구현된 함수의 간단한 사용법만을 확인하겠습니다. [3] 직관적으로 원의 형상을 띄는 위치에 하나의 blob을 의미하는 파랑색 동그라미를 생성하는 모습입니다. 잘못된 위치에 그려진 blob이 눈에 띄는데요, 이와 같은 결과를 parameter를 통해 handling하는 내용에 대해서 차후 다가올 주제인 Image Segmentation에서 확인하겠습니다.   ret, coin_img = cv2.threshold(coin, 200, 255, cv2.THRESH_BINARY_INV) params = cv2.SimpleBlobDetector_Params() params.minThreshold = 10 params.maxThreshold = 255 params.filterByArea = False params.filterByCircularity = False params.filterByConvexity = False params.filterByInertia = False detector = cv2.SimpleBlobDetector_create(params) # Blob detector 선언 keypoints = detector.detect(coin_img) # Blob labeling 수행 im_with_keypoints = \\ cv2.drawKeypoints(coin_img, keypoints, np.array([]), (0, 0, 255), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS) # 원본 이미지에 찾은 blob 그리기 plt.figure(figsize=(15,15)) plt.imshow(im_with_keypoints) plt.title('Coin keypoint', size=15) plt.show()    Reference  [1] 오일석, 컴퓨터 비전, 2014, pp. 67-75  [2] \u0026lsquo;Pixel connectivity\u0026rsquo;, Wikipedia. 2019 [Online]. Available: https://en.wikipedia.org/wiki/Pixel_connectivity  [3] Satya Mallick., \u0026lsquo;Blob Detection Using OpenCV ( Python, C++ )\u0026rsquo;, \u0026lsquo;Learn OpenCV. 2019 [Online]. Available: https://www.learnopencv.com/blob-detection-using-opencv-python-c/  ","date":1556716937,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556716937,"objectID":"75d1a8b06357b947ef3f8fe0505ec8b8","permalink":"/post/image-thresholding/","publishdate":"2019-05-01T22:22:17+09:00","relpermalink":"/post/image-thresholding/","section":"post","summary":"Image threshold using OpenCV","tags":[],"title":"2.3. Image Thresholding","type":"post"}]