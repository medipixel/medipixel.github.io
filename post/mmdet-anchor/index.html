<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.3.0">

  

  
  
  
  
  
  

  

  
  
  
    
  
  <meta name="description" content="Anchor algorithm in MMDetection">

  
  <link rel="alternate" hreflang="en-us" href="/post/mmdet-anchor/">

  


  

  

  

  

  

  

  
  
  
  <meta name="theme-color" content="#3f51b5">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.6a8804fae7a305c3da7b642ff90ede5e.css">

  

  
  
  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/post/mmdet-anchor/">

  
  
  
  
    
    
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Medipixel">
  <meta property="og:url" content="/post/mmdet-anchor/">
  <meta property="og:title" content="MMDetection - Anchor | Medipixel">
  <meta property="og:description" content="Anchor algorithm in MMDetection"><meta property="og:image" content="/img/icon-192.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2019-05-24T20:38:00&#43;09:00">
  
  <meta property="article:modified_time" content="2019-05-24T20:38:00&#43;09:00">
  

  

  

  <title>MMDetection - Anchor | Medipixel</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >
  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Medipixel</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        

        

        
        
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/">
            
            <span></span>
            
          </a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1 itemprop="name">MMDetection - Anchor</h1>

  

  
    



<meta content="2019-05-24 20:38:00 &#43;0900 KST" itemprop="datePublished">
<meta content="2019-05-24 20:38:00 &#43;0900 KST" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/young-kim/">young-kim</a></span>, <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/whi-kwon/">whi-kwon</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>May 24, 2019</time>
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    19 min read
  </span>
  

  
  

  
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=&amp;url="
         target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u="
         target="_blank" rel="noopener">
        <i class="fab fa-facebook-f"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=&amp;title="
         target="_blank" rel="noopener">
        <i class="fab fa-linkedin-in"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=&amp;title="
         target="_blank" rel="noopener">
        <i class="fab fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=&amp;body=">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      

<h2 id="introduction">Introduction</h2>

<p>SOTA 성능을 내는 많은 detector에서 anchor를 활용해서 bbox coordinate을 학습합니다. anchor가 무엇인지, 어떻게 one-stage, two-stage detector에서 사용되는지 살펴보겠습니다.</p>

<h2 id="import-libraries">Import Libraries</h2>

<ul>
<li>mmdet version: <a href="https://github.com/open-mmlab/mmdetection/tree/v0.6rc0" target="_blank">https://github.com/open-mmlab/mmdetection/tree/v0.6rc0</a></li>
</ul>

<pre><code class="language-python">import cv2
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as plticker
from matplotlib.lines import Line2D
from matplotlib.patches import Patch
import torch
import torch.nn.functional as F
</code></pre>

<h2 id="anchor-generate">Anchor Generate</h2>

<ul>
<li>code from: <a href="https://github.com/open-mmlab/mmdetection/blob/v0.6rc0/mmdet/core/anchor/anchor_generator.py" target="_blank">https://github.com/open-mmlab/mmdetection/blob/v0.6rc0/mmdet/core/anchor/anchor_generator.py</a></li>
</ul>

<pre><code class="language-python">def meshgrid(x, y):
    &quot;&quot;&quot;
    Args:
        x ():
        y ():
    &quot;&quot;&quot;
    xx = x.repeat(len(y))
    yy = y.view(-1, 1).repeat(1, len(x)).view(-1)
    return xx, yy

def gen_base_anchors(base_size, ratios, scales):
    w = base_size
    h = base_size

    x_ctr = 0.5 * (w - 1)
    y_ctr = 0.5 * (h - 1)

    h_ratios = torch.sqrt(ratios)
    w_ratios = 1 / h_ratios
    ws = (w * w_ratios[:, None] * scales[None, :]).view(-1)
    hs = (h * h_ratios[:, None] * scales[None, :]).view(-1)

    base_anchors = torch.stack(
        [
            x_ctr - 0.5 * (ws - 1), y_ctr - 0.5 * (hs - 1),
            x_ctr + 0.5 * (ws - 1), y_ctr + 0.5 * (hs - 1)
        ],
        dim=-1).round()
    return base_anchors

def grid_anchors(base_anchors, featmap_size, stride=16, device='cuda'):
    &quot;&quot;&quot;
    Args:
        base_anchors ():
        featmap_size ():
        stride (int):
        device (str)
    &quot;&quot;&quot;
    base_anchors = base_anchors.to(device)

    feat_h, feat_w = featmap_size
    shift_x = torch.arange(0, feat_w, device=device) * stride
    shift_y = torch.arange(0, feat_h, device=device) * stride
    shift_xx, shift_yy = meshgrid(shift_x, shift_y)
    shifts = torch.stack([shift_xx, shift_yy, shift_xx, shift_yy], dim=-1)
    shifts = shifts.type_as(base_anchors)

    all_anchors = base_anchors[None, :, :] + shifts[:, None, :]
    all_anchors = all_anchors.view(-1, 4)
    return all_anchors, shifts

def valid_flags(featmap_size, valid_size, num_base_anchors, device='cuda'):
    &quot;&quot;&quot;
    Args:
        featmap_size ():
        valid_size ():
        num_base_anchors ():
        device (str): 'cuda' or 'cpu'
    &quot;&quot;&quot;
    feat_h, feat_w = featmap_size
    valid_h, valid_w = valid_size
    assert valid_h &lt;= feat_h and valid_w &lt;= feat_w
    valid_x = torch.zeros(feat_w, dtype=torch.uint8, device=device)
    valid_y = torch.zeros(feat_h, dtype=torch.uint8, device=device)
    valid_x[:valid_w] = 1
    valid_y[:valid_h] = 1
    valid_xx, valid_yy = meshgrid(valid_x, valid_y)
    valid = valid_xx &amp; valid_yy
    valid = valid[:, None].expand(
        valid.size(0), num_base_anchors).contiguous().view(-1)
    return valid
</code></pre>

<ul>
<li>code from: <a href="https://github.com/open-mmlab/mmdetection/blob/f2cfa86b4294e2593429adccce64bfd049a27651/mmdet/models/anchor_heads/anchor_head.py#L89-L126" target="_blank">https://github.com/open-mmlab/mmdetection/blob/f2cfa86b4294e2593429adccce64bfd049a27651/mmdet/models/anchor_heads/anchor_head.py#L89-L126</a></li>
</ul>

<pre><code class="language-python">def get_anchors(image_shape, featmap_size, anchor_stride, scales, ratios, device='cuda'):
    &quot;&quot;&quot;
    Args:
        image_shape (list): [w, h, 3]
        featmap_size (list): [f_w, f_h]
        anchor_stride (int): normally w // f_w or h // f_h
        scales (torch.Tensor): scales of anchor
        ratios (torch.Tensor): ratios of anchor
        device (str): 'cuda' or 'cpu'

    Returns:
        anchors ():
        flags ():
    &quot;&quot;&quot;
    num_base_anchors = len(scales) * len(ratios)
    base_anchors = gen_base_anchors(base_size, ratios, scales)
    anchors, shifts = grid_anchors(base_anchors, featmap_size, anchor_stride, device)

    feat_h, feat_w = featmap_size
    h, w = image_shape[: 2]
    valid_feat_h = min(int(np.ceil(h / anchor_stride)), feat_h)
    valid_feat_w = min(int(np.ceil(w / anchor_stride)), feat_w)
    valid_size = [valid_feat_h, valid_feat_w]
    flags = valid_flags(featmap_size, valid_size, num_base_anchors, device)
    return anchors, flags
</code></pre>

<h2 id="anchor-gt-assign">Anchor -&gt; gt assign</h2>

<ul>
<li>code from: <a href="https://github.com/open-mmlab/mmdetection/blob/f2cfa86b4294e2593429adccce64bfd049a27651/mmdet/core/bbox/assigners/max_iou_assigner.py#L87-L146" target="_blank">https://github.com/open-mmlab/mmdetection/blob/f2cfa86b4294e2593429adccce64bfd049a27651/mmdet/core/bbox/assigners/max_iou_assigner.py#L87-L146</a></li>
</ul>

<pre><code class="language-python">def assign_wrt_overlaps(overlaps,
                        pos_iou_thr=0.5,
                        neg_iou_thr=0.4,
                        min_pos_iou=0.0):
    &quot;&quot;&quot;Assign w.r.t. the overlaps of bboxes with gts.

    Args:
        overlaps (torcch.Tensor): Overlaps between k gt_bboxes and n bboxes,
            shape(k, n).
        gt_labels (torch.Tensor, optional): Labels of k gt_bboxes, shape (k, ).

    Returns:
        :obj:`AssignResult`: The assign result.
    &quot;&quot;&quot;
    if overlaps.numel() == 0:
        raise ValueError('No gt or proposals')

    num_gts, num_bboxes = overlaps.size(0), overlaps.size(1)

    # 1. assign -1 by default
    assigned_gt_inds = overlaps.new_full(
        (num_bboxes, ), -1, dtype=torch.long)

    # for each anchor, which gt best overlaps with it
    # for each anchor, the max iou of all gts
    max_overlaps, argmax_overlaps = overlaps.max(dim=0)
    # for each gt, which anchor best overlaps with it
    # for each gt, the max iou of all proposals
    gt_max_overlaps, gt_argmax_overlaps = overlaps.max(dim=1)

    # 2. assign negative: below
    if isinstance(neg_iou_thr, float):
        assigned_gt_inds[(max_overlaps &gt;= 0)
                         &amp; (max_overlaps &lt; neg_iou_thr)] = 0
    elif isinstance(neg_iou_thr, tuple):
        assert len(neg_iou_thr) == 2
        assigned_gt_inds[(max_overlaps &gt;= neg_iou_thr[0])
                         &amp; (max_overlaps &lt; neg_iou_thr[1])] = 0

    # 3. assign positive: above positive IoU threshold
    pos_inds = max_overlaps &gt;= pos_iou_thr
    assigned_gt_inds[pos_inds] = argmax_overlaps[pos_inds] + 1

    # 4. assign fg: for each gt, proposals with highest IoU
    for i in range(num_gts):
        if gt_max_overlaps[i] &gt;= min_pos_iou:
            max_iou_inds = overlaps[i, :] == gt_max_overlaps[i]
            assigned_gt_inds[max_iou_inds] = i + 1

    return num_gts, assigned_gt_inds, max_overlaps

def bbox_overlaps(bboxes1, bboxes2):
    &quot;&quot;&quot;Calculate overlap between two set of bboxes.

    If ``is_aligned`` is ``False``, then calculate the ious between each bbox
    of bboxes1 and bboxes2, otherwise the ious between each aligned pair of
    bboxes1 and bboxes2.

    Args:
        bboxes1 (torch.Tensor): shape (m, 4)
        bboxes2 (torch.Tensor): shape (n, 4), if is_aligned is ``True``, then m and n
            must be equal.
        mode (str): &quot;iou&quot; (intersection over union) or iof (intersection over
            foreground).

    Returns:
        ious(Tensor): shape (m, n) if is_aligned == False else shape (m, 1)
    &quot;&quot;&quot;

    rows = bboxes1.size(0)
    cols = bboxes2.size(0)

    if rows * cols == 0:
        return bboxes1.new(rows, 1) if is_aligned else bboxes1.new(rows, cols)

    lt = torch.max(bboxes1[:, None, :2], bboxes2[:, :2])  # [rows, cols, 2]
    rb = torch.min(bboxes1[:, None, 2:], bboxes2[:, 2:])  # [rows, cols, 2]

    wh = (rb - lt + 1).clamp(min=0)  # [rows, cols, 2]
    overlap = wh[:, :, 0] * wh[:, :, 1]
    area1 = (bboxes1[:, 2] - bboxes1[:, 0] + 1) * (
        bboxes1[:, 3] - bboxes1[:, 1] + 1)

    area2 = (bboxes2[:, 2] - bboxes2[:, 0] + 1) * (
        bboxes2[:, 3] - bboxes2[:, 1] + 1)
    ious = overlap / (area1[:, None] + area2 - overlap)

    return ious
</code></pre>

<h2 id="delta-bbox-conversion">delta-bbox conversion</h2>

<ul>
<li>code from: <a href="https://github.com/open-mmlab/mmdetection/blob/f2cfa86b4294e2593429adccce64bfd049a27651/mmdet/core/bbox/transforms.py#L6-L68" target="_blank">https://github.com/open-mmlab/mmdetection/blob/f2cfa86b4294e2593429adccce64bfd049a27651/mmdet/core/bbox/transforms.py#L6-L68</a></li>
</ul>

<pre><code class="language-python">def bbox2delta(proposals, gt, means=[0, 0, 0, 0], stds=[1, 1, 1, 1]):
    &quot;&quot;&quot;convert bbox to delta

    Args:
        proposals (torch.Tensor): shape (m, 4)
        gt (torch.Tensor): shape (n, 4)
        means (list): for normalization, default [0, 0, 0, 0]
        stds (list): for normalization, default [1, 1, 1, 1]

    Returns:
        deltas (torch.Tensor): shape (m, 4)
    &quot;&quot;&quot;
    assert proposals.size() == gt.size()

    proposals = proposals.float()
    gt = gt.float()
    px = (proposals[..., 0] + proposals[..., 2]) * 0.5
    py = (proposals[..., 1] + proposals[..., 3]) * 0.5
    pw = proposals[..., 2] - proposals[..., 0] + 1.0
    ph = proposals[..., 3] - proposals[..., 1] + 1.0

    gx = (gt[..., 0] + gt[..., 2]) * 0.5
    gy = (gt[..., 1] + gt[..., 3]) * 0.5
    gw = gt[..., 2] - gt[..., 0] + 1.0
    gh = gt[..., 3] - gt[..., 1] + 1.0

    dx = (gx - px) / pw
    dy = (gy - py) / ph
    dw = torch.log(gw / pw)
    dh = torch.log(gh / ph)
    deltas = torch.stack([dx, dy, dw, dh], dim=-1)

    means = deltas.new_tensor(means).unsqueeze(0)
    stds = deltas.new_tensor(stds).unsqueeze(0)
    deltas = deltas.sub_(means).div_(stds)

    return deltas


def delta2bbox(rois,
               deltas,
               means=[0, 0, 0, 0],
               stds=[1, 1, 1, 1],
               max_shape=None):
    &quot;&quot;&quot;convert delta to bbox

    Args:
        rois (torch.Tensor): shape (m, 4)
        deltas (torch.Tensor): shape (n, 4)
        means (list): for unnormalization, default [0, 0, 0, 0]
        stds (list): for unnormalization, default [1, 1, 1, 1]
        max_shape (list): [w, h], normally image_shape

    Returns:
        bboxes (torch.Tensor): shape (n, 4)
    &quot;&quot;&quot;
    means = deltas.new_tensor(means).repeat(1, deltas.size(1) // 4)
    stds = deltas.new_tensor(stds).repeat(1, deltas.size(1) // 4)
    denorm_deltas = deltas * stds + means
    dx = denorm_deltas[:, 0::4]
    dy = denorm_deltas[:, 1::4]
    dw = denorm_deltas[:, 2::4]
    dh = denorm_deltas[:, 3::4]
    px = ((rois[:, 0] + rois[:, 2]) * 0.5).unsqueeze(1).expand_as(dx)
    py = ((rois[:, 1] + rois[:, 3]) * 0.5).unsqueeze(1).expand_as(dy)
    pw = (rois[:, 2] - rois[:, 0] + 1.0).unsqueeze(1).expand_as(dw)
    ph = (rois[:, 3] - rois[:, 1] + 1.0).unsqueeze(1).expand_as(dh)
    gw = pw * dw.exp()
    gh = ph * dh.exp()
    gx = torch.addcmul(px, 1, pw, dx)  # gx = px + pw * dx
    gy = torch.addcmul(py, 1, ph, dy)  # gy = py + ph * dy
    x1 = gx - gw * 0.5 + 0.5
    y1 = gy - gh * 0.5 + 0.5
    x2 = gx + gw * 0.5 - 0.5
    y2 = gy + gh * 0.5 - 0.5
    if max_shape is not None:
        x1 = x1.clamp(min=0, max=max_shape[1] - 1)
        y1 = y1.clamp(min=0, max=max_shape[0] - 1)
        x2 = x2.clamp(min=0, max=max_shape[1] - 1)
        y2 = y2.clamp(min=0, max=max_shape[0] - 1)
    bboxes = torch.stack([x1, y1, x2, y2], dim=-1).view_as(deltas)
    return bboxes
</code></pre>

<h2 id="loss">Loss</h2>

<ul>
<li>code from: <a href="https://github.com/open-mmlab/mmdetection/blob/f2cfa86b4294e2593429adccce64bfd049a27651/mmdet/core/loss/losses.py#L76-L89" target="_blank">https://github.com/open-mmlab/mmdetection/blob/f2cfa86b4294e2593429adccce64bfd049a27651/mmdet/core/loss/losses.py#L76-L89</a></li>
</ul>

<pre><code class="language-python">def smooth_l1_loss(pred, target, beta=1.0):
    &quot;&quot;&quot;
    Args:
        pred (torch.Tensor): (m, 4)
        target (torch.Tensor): (m, 4)
        beta (float): smooth l1 loss parameter, default 1.0

    Returns:
        loss (torch.Tensor): scalar tensor
    &quot;&quot;&quot;
    assert beta &gt; 0
    assert pred.size() == target.size() and target.numel() &gt; 0
    diff = torch.abs(pred - target)
    loss = torch.where(diff &lt; beta, 0.5 * diff * diff / beta,
                       diff - 0.5 * beta).sum()
    return loss


def binary_cross_entropy(pred, label):
    return F.binary_cross_entropy_with_logits(
        pred, label.float(), reduction='sum')
</code></pre>

<h2 id="prediction">Prediction</h2>

<pre><code class="language-python">def predict_pos_anchor(shape, target_deltas, sampled_neg_inds, seed=99):
    # predicted value
    torch.manual_seed(seed)
    pos_delta_pred = target_deltas + torch.rand(target_deltas.shape) / 5
    num_pos_neg_samples = target_deltas.shape[0] + len(sampled_neg_inds)
    pos_cls_pred = torch.rand(num_pos_neg_samples)
    return pos_cls_pred, pos_delta_pred
</code></pre>

<h2 id="visualize">Visualize</h2>

<pre><code class="language-python">def prepare_base_figure(shape, grid_size, figsize=(20, 20)):
    &quot;&quot;&quot;
    Args:
        shape (list):
        grid_size (int):
        figsize (tuple):
    &quot;&quot;&quot;
    fig, ax = plt.subplots(figsize=figsize)

    loc = plticker.MultipleLocator(base=grid_size)
    ax.xaxis.set_major_locator(loc)
    ax.yaxis.set_major_locator(loc)

    ax.grid(which='major', axis='both', linestyle='--', color='w')
    return ax


def draw_anchor_gt_overlaps(overlaps, gt_bboxes_list, featmap_size,
                            anchors_per_grid, anchor_stride,
                            grid_size=1, draw_gt=False, figsize=(20, 20)):
    &quot;&quot;&quot;Draw anchor overlaps w.r.t. gt bboxes

    Args:
        overlaps (torch.Tensor): shape (n, n, )
        gt_bboxes_list (torch.Tensor):
        anchors_per_grid (int):
        anchor_stride (int):
        grid_size (int):
        draw_gt (bool):
        figsize (tuple):
    &quot;&quot;&quot;
    max_anchor_overlaps = overlaps.reshape(*featmap_size, anchors_per_grid).cpu().numpy().max(axis=-1).copy()
    positive_overlaps = np.where(max_anchor_overlaps &gt; 0)

    for gt_bbox in gt_bboxes_list:
        assert any(gt_bbox % 16) is False

    grid_x, grid_y = max_anchor_overlaps.shape[:2]
    ax = prepare_base_figure(max_anchor_overlaps.shape, grid_size, figsize)
    title = &quot;Overlap(feature map's reg prediction and gt)&quot;

    if draw_gt:
        background_image = np.zeros([*max_anchor_overlaps.shape, 3])
        for gt_bbox in gt_bboxes_list:
            x1, y1, x2, y2 = (gt_bbox // anchor_stride).numpy().astype(int)
            cv2.rectangle(background_image, (x1, y1), (x2-1, y2-1), (0, 0, 255), 1)
        ax.imshow(background_image, extent=[0, grid_x, grid_y, 0])
        title += &quot;, [gt:blue square]&quot;

        legend_elements = [Line2D([0], [0], marker='s', color='b', markersize=20, lw= 0, label='gt')]
        ax.legend(handles=legend_elements, loc='upper right', fontsize=30)
    else:
        ax.imshow(max_anchor_overlaps, extent=[0, grid_x, grid_y, 0])
        title += &quot;, [overlaps:heatmap]&quot;

    for (x, y) in zip(*positive_overlaps):
        ax.annotate(f&quot;{max_anchor_overlaps[x, y]:.2f}&quot;, xy=(0, 0),
                    xytext=(x+0.13, y+0.6), color='white', size=20)
    ax.xaxis.tick_top()
    ax.tick_params(axis='both', which='major', labelsize=20)
    plt.margins(0)
    plt.title(title, fontsize=30, pad=20)
    plt.show()


def draw_pos_assigned_bboxes(image_shape, grid_size, gt_bboxes_list, pos_bboxes,
                             pos_pred_bboxes=None, figsize=(20, 20)):
    &quot;&quot;&quot;Draw positive, negative bboxes.

    Args:
        image_shape (list):
        grid_size (int):
        gt_bboxes_list (torch.Tensor):
        pos_bboxes (torch.Tensor):
        pos_pred_bboxes (torch.Tensor):
        figsize (tuple):
    &quot;&quot;&quot;
    assert len(pos_bboxes) == len(pos_pred_bboxes)
    for i in range(len(pos_bboxes)):
        background_image = np.zeros(image_shape)
        ax = prepare_base_figure(image_shape, grid_size, figsize)

        for gt_bbox in gt_bboxes_list:
            x1, y1, x2, y2 = gt_bbox
            gt_coord = [(x2 + x1) // 2, (y2 + y1) // 2, (x2 - x1) // 2, (y2 - y1) // 2]
            cv2.rectangle(background_image, (x1, y1), (x2, y2), (0, 0, 255), 1)

        x1, y1, x2, y2 = pos_bboxes[i]
        anchor_coord = [(x2 + x1) // 2, (y2 + y1) // 2, (x2 - x1) // 2, (y2 - y1) // 2]
        cv2.rectangle(background_image, (x1, y1), (x2, y2), (0, 255, 0), 1)

        x1, y1, x2, y2 = pos_pred_bboxes[i]
        pred_coord = [(x2 + x1) // 2, (y2 + y1) // 2, (x2 - x1) // 2, (y2 - y1) // 2]
        cv2.rectangle(background_image, (x1, y1), (x2, y2), (255, 0, 0), 1)

        legend_elements = [Line2D([0], [0], color='b', lw=4, label='gt'),
                           Line2D([0], [0], color='r', lw=4, label=f'pred(pos[{i}])'),
                           Line2D([0], [0], color='g', lw=4, label=f'anchor(pos[{i}])'),]

        image_x, image_y = image_shape[:2]

        ax.imshow(background_image, extent=[0, image_x, image_y, 0])
        ax.legend(handles=legend_elements, loc='upper right', fontsize=30)

        ax.annotate('coordination:', xy=(0, 0), xytext=(30, 150), color='white', size=30)
        ax.annotate(f'- anchor: {[int(x) for x in anchor_coord]}', xy=(0, 0),
                    xytext=(40, 160), color='green', size=30)
        ax.annotate(f'- gt: {[int(x) for x in gt_coord]}', xy=(0, 0),
                    xytext=(40, 170), color='blue', size=30)
        ax.annotate(f'- pred: {[int(x) for x in pred_coord]}', xy=(0, 0),
                    xytext=(40, 180), color='red',size=30)

        gt_coord = torch.tensor([gt_coord])
        anchor_coord = torch.tensor([anchor_coord])
        pred_coord = torch.tensor([pred_coord])
        gt_delta = bbox2delta(anchor_coord, gt_coord)
        pred_delta = bbox2delta(anchor_coord, pred_coord)

        ax.annotate('delta:', xy=(0, 0), xytext=(30, 190), color='white', size=30)
        ax.annotate(f'- anchor_gt: {[round(float(x), 2) for x in gt_delta[0]]}', xy=(0, 0),
                    xytext=(40, 200), color='white',size=30)
        ax.annotate(f'- anchor_pred: {[round(float(x), 2) for x in pred_delta[0]]}', xy=(0, 0),
                    xytext=(40, 210), color='white', size=30)
        ax.xaxis.tick_top()
        ax.tick_params(axis='both', which='major', labelsize=20)

        plt.title(&quot;gt, prediction, anchor&quot;, fontsize=30, pad=20)
        plt.show()
</code></pre>

<pre><code class="language-python">def draw_base_anchor_on_grid(base_anchor, figsize=(20, 20)):
    board = np.zeros((256, 256, 3))

    for anchor in base_anchor:
        x1, y1, x2, y2 = np.array(anchor) + 112  # 왜 112?
        cv2.rectangle(board, (x1, y1), (x2, y2), (0, 255, 0), 1)

    ax = prepare_base_figure((1, 1, 1), 16, figsize)

    ax.annotate('base anchor center at (0, 0)', xy=(0, 0), xytext=(20, -85), color='white',size=30,
                arrowprops=dict(facecolor='white', shrink=5),
                )

    ax.annotate('-23, -45', xy=(-23, -45), xytext=(-48, -78), color='white',size=30,
                arrowprops=dict(facecolor='white', shrink=5),
                )
    ax.annotate('-32, -32', xy=(-32, -32), xytext=(-68, -68), color='white',size=30,
                arrowprops=dict(facecolor='white', shrink=5),
                )
    ax.annotate('-45, -23', xy=(-45, -23), xytext=(-78, -58), color='white',size=30,
                arrowprops=dict(facecolor='white', shrink=5),
                )

    ax.annotate('23, 45', xy=(23, 45), xytext=(22, 72), color='white',size=30,
                arrowprops=dict(facecolor='white', shrink=5),
                )
    ax.annotate('32, 32', xy=(32, 32), xytext=(43, 43), color='white',size=30,
                arrowprops=dict(facecolor='white', shrink=5),
                )
    ax.annotate('45, 23', xy=(45, 23), xytext=(72, 22), color='white',size=30,
                arrowprops=dict(facecolor='white', shrink=5),
                )
    legend_elements = [Line2D([0], [0], color='g', lw=4, label='base anchor')]
    ax.imshow(board, extent=[-128, 128, 128, -128])
    ax.legend(handles=legend_elements, loc='upper right', fontsize=30)
    ax.xaxis.tick_top()
    ax.tick_params(axis='both', which='major', labelsize=20)
    plt.title(&quot;Base Anchor&quot;, fontsize=30, pad=20)
    plt.show()
</code></pre>

<pre><code class="language-python">def draw_anchor_samples_on_image(image_shape, base_anchor, all_anchors, shifts):
    board = np.zeros(image_shape)
    fig, ax = plt.subplots(figsize=(20, 20))
    loc = plticker.FixedLocator(range(0, image_shape[0], 16))

    ax.xaxis.set_major_locator(loc)
    ax.yaxis.set_major_locator(loc)

    ax.grid(which='major', axis='both', linestyle='--', color='w')

    for anchor in base_anchor:
        x1, y1, x2, y2 = np.array(anchor, dtype=np.uint8) + np.array([48, 112, 48, 112], dtype=np.uint8)
        cv2.rectangle(board, (x1, y1), (x2, y2), (0, 255, 0), 1)
        x1 = int(x1 + 128)
        x2 = int(x2 + 128)
        cv2.rectangle(board, (x1, y1), (x2, y2), (0, 0, 255), 1)

    for i in range(64, 208, 16):
        ax.scatter(i, 128, s=50, c='r')

    legend_elements = [Line2D([0], [0], color='g', lw=4, label='anchor[4, 8]'),
                       Line2D([0], [0], marker='o', color='r', label='center points of anchors',
                       markerfacecolor='r', lw=0, markersize=12),
                       Line2D([0], [0], color='b', lw=4, label='anchor[12, 8]')]

    ax.annotate('coords: (4x16, 8x16)\nindex: (4, 8, :3)', xy=(63, 127), xytext=(30, 70), color='white',size=30,
                arrowprops=dict(facecolor='white', shrink=5),
                )
    ax.annotate('8 shift steps along x-axis', xy=(80, 128), xytext=(64, 145), color='white',size=25)
    ax.annotate('', xy=(188, 133), xytext=(64, 133), color='white',size=30,
                arrowprops=dict(facecolor='white', shrink=5),
                )
    ax.imshow(board, extent=[0, image_shape[0], image_shape[1], 0])
    ax.legend(handles=legend_elements, loc='upper right', fontsize=30)
    ax.xaxis.tick_top()
    ax.tick_params(axis='both', which='major', labelsize=20)
    plt.title(&quot;Anchor Samples on Image&quot;, fontsize=30, pad=20)
    plt.show()
</code></pre>

<h2 id="what-is-anchor">What is Anchor?</h2>

<ul>
<li>첫 제안: Anchor라는 개념은 <a href="https://arxiv.org/abs/1506.01497.pdf" target="_blank">Faster R-CNN</a>에서 처음으로 제안되었습니다.</li>
<li>주요 모델: Anchor는 대부분의 one-stage, two-stage detector에서 사용하며 대표적으로는 <a href="https://arxiv.org/abs/1708.02002.pdf" target="_blank">RetinaNet</a>(one-stage)와 Faster R-CNN(two-stage)가 존재합니다.</li>
<li>목적:

<ul>
<li>detection은 image에 object가 있는 영역을 bounding box(bbox)로 예측해야 합니다. 이런 예측을 용이하게 해주기 위해서 이미지로부터 얻은 feature map의 매 pixel 위치마다 bbox를 여러 개를 그립니다.(anchor) 이 anchor들과 gt를 비교하고 겹치는 영역을 기준으로 학습 대상으로 사용할 anchor를 선별합니다.</li>
<li>선별한 anchor를 이용해서 <strong>anchor와 정답(ground-truth)과의 차이</strong>에 대해서 학습합니다.(이 때, anchor의 크기가 적절하지 못한 경우에는 차이의 편차가 커지게 될 것이므로 학습이 어려워질 수 있어서 적절한 크기를 선정하는게 중요합니다.)</li>
<li>anchor는 균일한 간격, 일정한 규칙으로 생성되어, 물체가 특정 위치에 존재할 때만 탐지가 잘 되거나, 혹은 특정 위치에서는 탐지가 잘 되지 않는 현상을 줄입니다. 이를 translation-Invariance라고 합니다. [1]</li>
</ul></li>
<li>Parameters:

<ul>
<li>scale: feature map에서의 anchor 크기(scale)입니다.</li>
<li>ratio: feature map에서의 anchor 비율(ratio)입니다.</li>
<li>stride: image를 기준으로 어느 정도 간격으로 anchor를 생성할 것인지 나타내는 값입니다.(주로 image와 feature map 크기의 비율 값을 사용합니다.)

<ul>
<li>scale과 ratio가 feature map 내에서의 <code>base_anchor_size</code>를 만들게 됩니다.</li>
<li>feature map의 크기는 image의 너비, 높이를 <code>stride</code>로 나눈 값이기 때문에 이게 반영된 image에서의 anchor 크기는 <code>base_anchor_size * stride</code> 입니다.</li>
</ul></li>
</ul></li>
</ul>

<h2 id="how-to-draw-grid-anchors">How to draw grid anchors</h2>

<ul>
<li>1개 anchor bbox의 coordination은 <code>[x1, y1, x2, y2]</code>로 표현할 수 있습니다.</li>
<li>anchor는 feature map의 예측 값에 매칭되어야 하기 때문에 feature map과 동일한 width, height를 가지며 channel은 4로 갖습니다.</li>
<li><code>base_anchor</code>는 기본적인 anchor의 모습입니다.</li>
<li>feature map과 동일한 width, height를 갖더라도 실제 이미지 상 크기에서 anchor가 어디에 위치하는지를 알 수 있어야 합니다. 그래서 stride를 고려합니다.

<ul>
<li>stride를 <code>[image_width // feature_map_width] == [image_height // feature_map_height]</code>로 지정하는 경우에 image와 feature map 비율만큼의 크기를 anchor의 1개 pixel이 가지게 됩니다. 즉, image에서 생각을 하면 stride만큼 띄어서 anchor가 존재한다고 생각하시면 됩니다.(<code>grid_anchors</code>)</li>
<li>중심 좌표가 stride 만큼 떨어져서 존재한다고 보면 되고, 그 위에 그려지는 bbox의 크기는 <code>base_anchor_size</code>(<code>AnchorGenerator.base_anchors</code>)가 결정하게 됩니다. scale, ratio 2개 parameter로 결정되는 크기이고 크기의 단위는 <strong>1 stride</strong>가 됩니다. [2]</li>
</ul></li>
<li>RetinaNet의 경우 Octave scale을 사용하였습니다. Faster R-CNN에서 사용한, $2,4,6$ 등 $n$배로 올라가는 scale 간격 대신 $2^0, 2^{\frac 1 3}, 2^{\frac 2 3}$과 같이 (base scale)^(octave scale)을 사용하였습니다. [3]</li>
<li><code>base_anchor_size</code>는 scale, ratio에 의해 결정되어 feature map에 동일하게 적용됩니다. 하지만, feature map이 작은 경우, stride가 커지게 되고 반대의 경우엔 stride가 작아지게 되어 image에서의 anchor bbox 크기는 feature map의 영향을 받습니다.

<ul>
<li>anchor box가 크다는 건, 큰 물체를 잡는데 유리할 것이고 anchor box가 작은 경우엔 작은 물체를 잡는데 유리할 것입니다.</li>
<li>이는 feature map의 크기에 따라서 예측하는 물체의 크기와도 상관이 있습니다.(보통 CNN에서의 큰 feature map이 high-level 정보를 가지고 있어서 큰 물체를 예측 잘 하고, 작은 feature가 low-level 정보를 다뤄서 작은 물체 예측을 잘 한다고 알려져 있습니다.)</li>
</ul></li>
</ul>

<h3 id="settings">Settings</h3>

<ul>
<li><code>gt_bboxes_list</code> 의 bbox 크기를 크게 잡으면 다양한 positive anchor 후보들이 생기는 것을 확인할 수 있습니다.</li>
<li>scale, ratio를 조절해서 anchor bbox의 형태를 편향되게 만들 수 있습니다.</li>
<li><code>base_size</code>는 <code>image_size</code> // <code>featmap_size</code>의 값을 주로 갖는데, 이보다 크거나 작으면 전체 image를 커버하지 못하거나 image를 넘어서 커버하게 될 수 있습니다.</li>
</ul>

<pre><code class="language-python"># 1개 feature map에 대해서만 분석을 진행합니다.
# multi-level feature map(FPN)이 사용되는 경우는 뒤에서 보도록 하겠습니다.

base_size = 32
anchor_stride = 32
scales = torch.Tensor([2, 4, 8])
ratios = torch.Tensor([0.5, 1.0, 2.0])
featmap_size = [16, 16]
device = 'cpu'
image_shape = [256, 256, 3]
anchors_per_grid = len(scales) * len(ratios)

# x1y1x2y2
gt_bboxes_list = torch.FloatTensor([[32, 32, 32*3, 32*3]]).to(device)
</code></pre>

<h3 id="base-anchor">Base Anchor</h3>

<ul>
<li>base anchor를 원점이 중심인 좌표계에 그려봅니다. 이 base anchor는 scale * ratio의 개수만큼 생성되며 feature map 각 pixel의 해당 위치에 존재하게 됩니다.</li>
</ul>

<pre><code class="language-python">base_anchor = gen_base_anchors(base_size, ratios, scales[:1])
draw_base_anchor_on_grid(base_anchor)
</code></pre>

<pre><code>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
</code></pre>




  

<figure>

<img src="/img/mmdet%20-%20anchor_24_1.png" />


</figure>

<ul>
<li>각 feature map의 pixel은 원 image 기준 좌표가 있을 것입니다. 이 좌표들에 base anchor를 더해주면 feature map 기준 각 pixel에 base anchor가 존재하게 되고 image 기준으로 stride 만큼 띄엄띄엄 base anchor가 존재하는 것으로 해석할 수 있습니다.

<ul>
<li>feature map 모든 pixel에 할당된 anchor와 이 때 anchor들 간 거리인 <code>shifts</code>를 구하고 몇 개의 샘플을 시각화해봅시다.</li>
</ul></li>
</ul>

<pre><code class="language-python">all_anchors, shifts = grid_anchors(base_anchor, featmap_size, anchor_stride, device)
draw_anchor_samples_on_image(image_shape, base_anchor, all_anchors, shifts)
</code></pre>

<pre><code>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
</code></pre>




  

<figure>

<img src="/img/mmdet%20-%20anchor_26_1.png" />


</figure>

<ul>
<li><code>get_anchors</code>라는 함수를 통해 전체 anchor와 각각에 대한 valid 여부를 나타내는 flag를 얻습니다.</li>
</ul>

<pre><code class="language-python">anchors, flags = get_anchors(image_shape, featmap_size, anchor_stride, scales, ratios, device)
</code></pre>

<pre><code class="language-python">assert anchors.shape[0] == featmap_size[0] * featmap_size[1] * 9  # feature map 32x32 각 pixel에 9개의 anchors
assert len(flags) == len(anchors)  # anchor를 사용할 지 말지 결정하는 flags와 anchors의 개수는 같아야 합니다.
</code></pre>

<h2 id="anchor-selection">Anchor Selection</h2>

<ul>
<li>anchor는 gt와의 overlap 정도에 따라서 positive, negative를 배정합니다. 이 배정된 값들은 이 후에 regression, classification에 활용되게 됩니다.

<ul>
<li>positive는 classification, regression 모두에 활용됩니다. 그래야 특정 bbox에 대해서 object의 class와 영역을 예측할 수 있게 됩니다.</li>
<li>negative는 classification에만 활용됩니다. 그 이유는 negative의 경우 background라는 정보는 가지고 있지만, 어느 위치에 물체가 있다는 정보는 가지고 있지 않기 때문입니다. [4]</li>
<li>overlap은 <a href="https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/" target="_blank">IoU(Intersection over Union)</a>를 통해 계산합니다.</li>
</ul></li>
</ul>

<pre><code class="language-python">overlaps = bbox_overlaps(gt_bboxes_list, anchors)
</code></pre>

<pre><code class="language-python">assert overlaps.shape == (len(gt_bboxes_list), anchors.shape[0])
</code></pre>

<pre><code class="language-python">draw_anchor_gt_overlaps(overlaps, gt_bboxes_list, featmap_size,
                        anchors_per_grid, anchor_stride=anchor_stride, draw_gt=True)
</code></pre>

<pre><code>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
</code></pre>




  

<figure>

<img src="/img/mmdet%20-%20anchor_33_1.png" />


</figure>

<pre><code class="language-python">draw_anchor_gt_overlaps(overlaps, gt_bboxes_list, featmap_size, anchors_per_grid, anchor_stride)
</code></pre>




  

<figure>

<img src="/img/mmdet%20-%20anchor_34_0.png" />


</figure>

<pre><code class="language-python"># gt와의 overlap에 따라 pos, negative를 배정.
num_gts, assigned_gt_inds, max_overlaps = assign_wrt_overlaps(overlaps)
</code></pre>

<pre><code class="language-python">pos_inds = torch.nonzero(assigned_gt_inds &gt; 0).squeeze(-1).unique()  # positive indices
neg_inds = torch.nonzero(assigned_gt_inds == 0).squeeze(-1).unique()  # negative indices
</code></pre>

<pre><code class="language-python"># positive와 1:1 비율로 학습에 사용할 negative sample을 얻습니다.
sampled_neg_inds = neg_inds[torch.randint(0, len(neg_inds), size=(len(pos_inds), ))]
</code></pre>

<pre><code class="language-python">pos_neg_cls_label = torch.cat([torch.ones(len(pos_inds)), torch.zeros(len(sampled_neg_inds))])
</code></pre>

<pre><code class="language-python">bboxes = anchors  # bboxes
pos_bboxes = bboxes[pos_inds]  # positive boxes
pos_assigned_gt_inds = assigned_gt_inds[pos_inds] - 1
pos_gt_bboxes = gt_bboxes_list[pos_assigned_gt_inds, :]
</code></pre>

<h2 id="anchor-as-a-target">Anchor as a Target</h2>

<ul>
<li><strong>gt-anchor 차이</strong>에 대해서 학습해야 하기 때문에 [5] anchor bbox를 coordination(<code>[x1, y1, x2, y2]</code>) 형태에서 target(<code>target_delta</code>)으로 변환해주는 과정을 거쳐야 합니다.</li>
</ul>

<pre><code class="language-python"># target_deltas는 특정 pos_inds에 대한 것이며 이 inds에 할당된 anchor를 기준으로만 loss가 계산이 되도록 해야 합니다.
target_deltas = bbox2delta(pos_bboxes, pos_gt_bboxes)
</code></pre>

<pre><code class="language-python"># check if delta has same bbox value when reversed
bboxes_reversed = delta2bbox(pos_bboxes, target_deltas)
assert torch.equal(bboxes_reversed[0], gt_bboxes_list[0])
</code></pre>

<h2 id="train-anchor">Train anchor</h2>

<ul>
<li>anchor target을 만들었다면 앞에서 나온 feature를 network(<code>anchor_head</code>)를 통과시켜 reg_pred로 delta를 예측하도록, score로 class를 예측하도록 학습시키면 됩니다.</li>
<li>loss는 one/two-stage network 마다 다르게 적용되나 공통적으로 regression은 smooth-l1를, classification은 <a href="https://en.wikipedia.org/wiki/Cross_entropy" target="_blank">cross entropy</a>를 가장 많이 사용합니다.</li>

<li><p>loss 계산에는 positive, negative sample을 모두 다 사용할 수는 있지만, positive sample에 비해 negative sample의 개수가 압도적으로 많으므로, 일부 정해진 숫자 만큼만의 sample을 선정하여 학습에 사용합니다.(e.g. positive:negative=1:1.) [5]</p></li>

<li><p>엄밀하게는 anchor prediction을 구하고 그 중에 <code>pos_inds</code>에 해당하는 값만 가져오는 과정을 거쳐야 하는데 편의를 위해서 해당 과정을 거쳐서 <code>pos_delta_pred</code>를 구했다고 하겠습니다.</p></li>
</ul>

<pre><code class="language-python"># delta prediction for positive indices (will update weight)
pos_neg_cls_pred, pos_delta_pred = predict_pos_anchor(anchors.shape, target_deltas, sampled_neg_inds)
</code></pre>

<pre><code class="language-python">reg_loss = smooth_l1_loss(pos_delta_pred, target_deltas, beta=1.0)
print(&quot;reg_loss:&quot;, reg_loss)
cls_loss = binary_cross_entropy(pos_neg_cls_pred, pos_neg_cls_label)
print(&quot;cls_loss:&quot;, cls_loss)
</code></pre>

<pre><code>reg_loss: tensor(0.0795)
cls_loss: tensor(2.7997)
</code></pre>

<h2 id="test">Test</h2>

<ul>
<li>feature map을 받아 bbox의 cls_pred, reg_pred를 예측할 때 <strong>reg_pred를 delta로 하기 때문에</strong>, delta를 bbox로 변환해주는 과정이 필요합니다.(<code>delta2bbox</code>)</li>
<li>delta는 <strong>gt-anchor의 차이</strong>이기 때문에 anchor bbox의 coordination 정보를 가지고 있으면 재변환해주는 과정은 수식적으로 풀기만 하면 되어 어렵지 않습니다.</li>

<li><p>최종적으로 object 예측 결과는 cls_pred가 특정 threshold 이상인 값들에 대해서 <a href="https://en.wikipedia.org/wiki/Canny_edge_detector#Non-maximum_suppression" target="_blank">Non-maximum suppresion(NMS)</a>를 통과시킨 결과입니다.</p></li>

<li><p><code>cls_pred</code> threshold, nms가 모두 고려되었다고 가정하고 위에서 얻은 <code>pos_delta_pred</code>를 test 결과로 얻었다고 가정하겠습니다.</p></li>
</ul>

<pre><code class="language-python">pos_bboxes_pred = delta2bbox(pos_bboxes, pos_delta_pred)
</code></pre>

<pre><code class="language-python">print(&quot;gt_bboxes_list:&quot;, gt_bboxes_list)
print(&quot;pos_bboxes_pred:&quot;, pos_bboxes_pred)
</code></pre>

<pre><code>gt_bboxes_list: tensor([[32., 32., 96., 96.]])
pos_bboxes_pred: tensor([[ 26.1650,  42.4359, 103.7359, 120.4897],
        [ 37.5533,  37.3444, 102.2102, 107.5841]])
</code></pre>

<pre><code class="language-python"># blue: gt, green: anchor, red: prediction32, 32, 9
draw_pos_assigned_bboxes(image_shape, base_size, gt_bboxes_list, pos_bboxes, pos_bboxes_pred)
</code></pre>

<pre><code>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
</code></pre>




  

<figure>

<img src="/img/mmdet%20-%20anchor_51_1.png" />


</figure>

<pre><code>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
</code></pre>




  

<figure>

<img src="/img/mmdet%20-%20anchor_51_3.png" />


</figure>

<h2 id="reference">Reference</h2>

<hr />

<h3 id="faster-r-cnn">Faster R-CNN</h3>

<p><strong>[1]</strong> Translation-Invariant Anchors
     An important property of our approach is that it is translation invariant, both in terms of the anchors and the functions that compute proposals relative to the anchors.
     If one translates an object in an image, the proposal should translate and the same function should be able to predict the proposal in either location.</p>

<p><strong>[2]</strong> Multi-Scale Anchors as Regression References
     Our design of anchors presents a novel scheme for addressing multiple scales (and aspect ratios).
     The second way is to use sliding windows of multiple scales (and/or aspect ratios) on the feature maps.</p>

<p><strong>[3]</strong> For denser scale coverage than in Faster-RCNN, at each level we add anchors of sizes ${2^0,2^{\frac 1 3}, 2^{\frac 2 3}}$ of the original set of $3$ aspect ratio anchors.
     This improve AP in our setting. In total there are $A = 9$ anchors per level and across levels they cover the scale range $32 - 813$ pixels with respect to the network’s input image.</p>

<p><strong>[4]</strong> For training RPNs, we assign a binary class label (of being an object or not) to each anchor.
     We assign a positive label to two kinds of anchors:
     (i) the anchor/anchors with the highest Intersection-overUnion (IoU) overlap with a ground-truth box, or
     (ii) an anchor that has an IoU overlap higher than $0.7$ with any ground-truth box.
     Note that a single ground-truth box may assign positive labels to multiple anchors.
     We assign a negative label to a non-positive anchor if its IoU ratio is lower than $0.3$ for all ground-truth boxes.
     Anchors that are neither positive nor negative do not contribute to the training objective.</p>

<p><strong>[5]</strong> For bounding box regression, we adopt the parameterizations of the 4 coordinates following:
$$ t_x = (x - x_a) / w_a,\ t_y = (y - y_a) / h_a, <br />
   t_w = \log(w / w_a),\ t_h = \log(h / h_a), <br />
   t_x^{\ast} = (x^{\ast} - x_a) / w_a,\ t_y^{\ast} = (y^{\ast} - y_a) / h_a, <br />
   t_w^{\ast} = \log(w^{\ast} / w_a),\ t_h^{\ast} = \log(h^{\ast} / h_a) $$</p>

<p><strong>[6]</strong> It is possible to optimize for the loss functions of all anchors, but this will bias towards negative samples as they are dominate.
     Instead, we randomly sample $256$ anchors in an image to compute the loss function of a mini-batch, where the sampled positive and negative anchors have a ratio of up to $1:1$.</p>

<h3 id="retinanet">RetinaNet</h3>

<p><strong>[7]</strong> The design of our RetinaNet detector shares many similarities with previous dense detectors, in particular the concept of &lsquo;anchors&rsquo; introduced by RPN and use of features pyramids as in SSD and FPN.</p>

<p><strong>[8]</strong> We use translation-invariant anchor boxes similar to those in the RPN variant. The anchors have areas of $32^2$ to $512^2$ on pyramid levels $P_3$ to $P_7$, respectively. at each pyramid level we use anchors at three aspect ratios ${1:2, 1:1, 2:1}$. For denser scale coverage, at each level we add anchors of sizes ${2^0, 2^{\frac 1 3}, 2^{\frac 2 3}}$ of the original set of 3 aspect ratio anchors. This improve AP in our setting. In total there are $A=9$ anchors per level and across levels they cover the scale range $32-813$ pixels with respect to the network&rsquo;s input image. Each anchor is assigned a length $K$ one-hot vector of classification targets, where $K$ is the number of object classes, and a $4$-vector of box regression targets. We use the assignment rule from RPN but modified for multi-class detection and with adjusted thresholds. Specifically, anchors are assigned to ground-truth object boxes using an intersection-over-union(IoU) threshold of $0.5$; and to background if their IoU is in $[0, 0.4)$. As each anchor is assigned to at most one object box, we set the corresponding entry in its length $K$ label vector to $1$ and all other entries to $0$. If an anchor is unassigned, which may happen with overlap in $[0.4, 0.5)$, it is ignored during training. Box regression targets are computed as the offset between each anchor and its assigned object box, or omitted if there is no assignment.</p>

<p><strong>[9]</strong> The classification subnet predicts the probability of object presence at each spatial position for each of the $A$ anchors and $K$ object classes.</p>

<p><strong>[10]</strong> In parallel with the object classification subnet, we attach another small FCN to each pyramid level for the purpose of regressing the offset from each anchor box to a nearby ground-truth object, if one exists. For each of the $A$ anchors per spatial location, these $4$ outputs predict the relative offset between the anchor and the ground-truth box.</p>

    </div>

    



    
      








  
  
    
  
  






	
	
	
	
	
	<div class="media author-card" itemscope itemtype="http://schema.org/Person">
	  
	  
	  <img class="portrait mr-3" src="/author/young-kim/avatar_hu52a603635ecebd45650b162dadabb4e5_12861_250x250_fill_q90_lanczos_center.jpg" itemprop="image" alt="Avatar">
	  
	  <div class="media-body">
	    <h5 class="card-title" itemprop="name"><a href="/authors/young-kim">YoungEon Kim</a></h5>
	    <h6 class="card-subtitle">A.I Scientist</h6>
	    <p class="card-text" itemprop="description">My research interests include distributed robotics, mobile computing and programmable matter.</p>
	    <ul class="network-icon" aria-hidden="true">
	      
	      
	      
	      
		
	      
	      
	      
	      
	      
	      <li>
		<a itemprop="sameAs" href="mailto:young.kim@medipixel.io" >
		  <i class="fas fa-envelope"></i>
		</a>
	      </li>
	      
	      
	      
	      
		
	      
	      
	      
	      
	      
		
	      
	      <li>
		<a itemprop="sameAs" href="https://twitter.com/GeorgeCushen" target="_blank" rel="noopener">
		  <i class="fab fa-twitter"></i>
		</a>
	      </li>
	      
	      
	      
	      
		
	      
	      
	      
	      
	      
		
	      
	      <li>
		<a itemprop="sameAs" href="https://github.com/kye9216789" target="_blank" rel="noopener">
		  <i class="fab fa-github"></i>
		</a>
	      </li>
	      
	    </ul>
	  </div>
	</div>
	
	



	
	
	
	
	
	<div class="media author-card" itemscope itemtype="http://schema.org/Person">
	  
	  
	  <img class="portrait mr-3" src="/author/whi-kwon/avatar_hu52a603635ecebd45650b162dadabb4e5_12861_250x250_fill_q90_lanczos_center.jpg" itemprop="image" alt="Avatar">
	  
	  <div class="media-body">
	    <h5 class="card-title" itemprop="name"><a href="/authors/whi-kwon">Whi Kwon</a></h5>
	    <h6 class="card-subtitle">AI Researcher</h6>
	    
	    <ul class="network-icon" aria-hidden="true">
	      
	      
	      
	      
		
	      
	      
	      
	      
	      
	      <li>
		<a itemprop="sameAs" href="mailto:whi.kwon@medipixel.io" >
		  <i class="fas fa-envelope"></i>
		</a>
	      </li>
	      
	      
	      
	      
		
	      
	      
	      
	      
	      
		
	      
	      <li>
		<a itemprop="sameAs" href="https://github.com/whikwon" target="_blank" rel="noopener">
		  <i class="fab fa-github"></i>
		</a>
	      </li>
	      
	    </ul>
	  </div>
	</div>
	
	


      
      
    

    

    


  </div>
</article>

<div class="container">
  <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

</div>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/python.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.3258b3a711acd6208568ec000de4beec.js"></script>

  </body>
</html>

